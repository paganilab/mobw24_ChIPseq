[
  {
    "objectID": "pages/Day1.html",
    "href": "pages/Day1.html",
    "title": "Day 1",
    "section": "",
    "text": "Introduction to ChIP-seq\nSetting up R environment\nUnderstanding ChIP-seq (and NGS) data core processing\nDataset introduction and exploration"
  },
  {
    "objectID": "pages/Day1.html#objectives",
    "href": "pages/Day1.html#objectives",
    "title": "Day 1",
    "section": "",
    "text": "Introduction to ChIP-seq\nSetting up R environment\nUnderstanding ChIP-seq (and NGS) data core processing\nDataset introduction and exploration"
  },
  {
    "objectID": "pages/Day1.html#omics-experiments",
    "href": "pages/Day1.html#omics-experiments",
    "title": "Day 1",
    "section": "Omics experiments",
    "text": "Omics experiments\n‚ÄòOmics‚Äô as disciplines work to answer questions regarding specific ‚Äòomes‚Äô. These questions are enabled by omics experiments; in most cases, these will be sequencing- or mass-spectrometry based.\nThe identification of a suitable model, types and numbers of samples, the process of extraction and purification of our target ‚Äòome‚Äô, and finally its reformulation into a sequenceable product, i.e.¬†a library, shape the final bioinformatic dataset and place limits on the possible questions we will be able to ask of it.\n\nFor example, in the case of RNA-seq, phenol-based extraction and precipitation will yield results that are not always comparable with total RNA extraction with a spin column. Even more, the contingent decision to perform ribodepletion or enrichment for polyadenylated transcript will have an impact even on simple gene expression analyses.\n\nSumming up, an Omics experiment is designed starting from a question, selecting the sample(s), wet lab procedure and library preparations that will enable the most truthful, informative dataset to be built.\n\n\n\n\nDesigning Omics experiments: from question to dataset"
  },
  {
    "objectID": "pages/Day1.html#the-chip-seq-technology",
    "href": "pages/Day1.html#the-chip-seq-technology",
    "title": "Day 1",
    "section": "The ChIP-seq technology",
    "text": "The ChIP-seq technology\nIn this context, we chose to deal with a Chromatin Immuno-precipitation (ChIP from now on) assay.¬†\n\nüí≠ Suppose you have found an interesting, new histone variant, and managed to produce a strong antibody against it. You want to know where it‚Äôs located in the genome of your cells of interest (say, tumor cells), to start deducing its function.\n\nChIP, coupled to sequencing (ChIP-seq), allows you to approach the question, genome-wide.\nIt works by extracting chromatin (DNA bound to protein), fragmenting it, and selecting the protein or protein variant of interest from this complex mixture with an antibody in some way specific for it. The subset of genome you will have isolated will constitute the sequences you expect your variant to bind, informing your inferences regarding its function.\n\nChIP-seq use in mapping histone modifications has been seminal in epigenetics research! (Ku et al., 2011)\n\nHere you can see a recap of the major steps:\n+ Cross-linking: add Formaldehyde\n+ Cell lysis (breakdown of cell membranes)\n+ Fragmentation of DNA fragments (shearing), e.g. digestion with specific endonuclease enzymes (dpn1) or sonication\n+ Addition of specific bead-coupled antibody against the protein of interest, separating it from the mixture\n+ Separate DNA from the protein (reverse cross-linking), and DNA purification\n+ DNA fragment amplification, library construction\n+ Library sequencing by NGS (essentially Illumina)"
  },
  {
    "objectID": "pages/Day1.html#tips-for-designing-a-chip-seq-experiment",
    "href": "pages/Day1.html#tips-for-designing-a-chip-seq-experiment",
    "title": "Day 1",
    "section": "Tips for designing a ChIP-seq experiment",
    "text": "Tips for designing a ChIP-seq experiment\nBesides characterizing binding patterns in general, a common goal in ChIP-seq experiments is to identify changes in protein binding patterns between conditions, an application known as differential binding.\nEffective analysis of ChIP-seq data requires sufficient coverage by sequence reads (sequencing depth). It mainly depends on the size of the genome, and the number and size of the binding sites of the protein.\n\nFor mammalian (or any species with a relatively large genome) transcription factors (TFs) and chromatin modifications such as enhancer-associated histone marks: 20 million reads are adequate\nProteins with more binding sites (e.g., RNA Pol II) or more promiscuous factors (e.g.¬†CTCF) need more reads, up to 60 million for mammalian ChIP-seq\nSequencing depth rules of thumb: &gt;10M reads for narrow peaks, &gt;20M for broad peaks (you will understand better this concept later in the Peak Calling section)\n\nAnother consideration is about sequence modality: long & paired-end reads are useful, but not essential - they may however help in specific, high-precision binding site discovery. Most of the times, ChIP-seq can be performed in single-end mode.\nAbout the number of technical replicates, unlike with RNA-Seq, more than two replicates do not significantly increase the number of identified peaks.\n\nChIP controls\nA successful ChIP-seq experiment must have the appropriate controls to optimize signal-noise ratio. Well-made controls help eliminate false positives. In fact, there is always some signal on open chromatin. This may be due to multiple factors, but can be boiled down to three reasons: A) non-specific binding from the antibody can precipitate random sequences B) digestion, sonication, or any other method of shearing will introduce biases towards certain sequences C) different operators will perform the same protocol slightly differently, yielding higher or lower signal/noise ratios.\nThe following controls are often used to abate these biases:\n\nAn ‚Äúinput‚Äù DNA sample in which a library is generated from DNA that has been cross-linked and sonicated but not immuno-precipitated. It controls for the bias introduced before even immunoprecipitating, i.e.¬†from sonication.\nAn IgG ‚Äúmock‚Äù-ChIP, using an antibody that will not bind to nuclear proteins and should generate random immuno-precipitated DNA. It controls for non-specific sequences that precipitate alongside the target protein, but are not bound by it.\nA Spike-In can also be introduced, using different genomes (e.g.¬†Drosophila in a Human experiment) to control for handling variability between users, or batch differences between reagents. This helps dampen these confounders.\n\n\nInput DNA control is ideal in most of the cases. Indeed, one problem with IgG control is that if too little DNA is recovered after immuno-precipitation, sequencing library will be of low complexity and binding sites identified using this control could be biased. Read this biostars post for discussion."
  },
  {
    "objectID": "pages/Day1.html#next-generation-sequencing",
    "href": "pages/Day1.html#next-generation-sequencing",
    "title": "Day 1",
    "section": "Next Generation Sequencing",
    "text": "Next Generation Sequencing\nNGS technologies (Illumina/PacBio/Ultima/BGI) allow the processing of millions of synthesis reactions in parallel, resulting in high throughput, higher sensitivity, speed and reduced cost compared to first generation sequencing technologies (e.g., Sanger method).\n\nGiven the vast amounts of quantitative sequencing data generated, NGS-based methods rely on resource-intensive data processing pipelines to analyze data.\n\nIn addition to the sequence itself, and unlike Sanger sequencing, the high-throughput nature of NGS provides quantitative information (depth of coverage) due to the high level of sequence redundancy at a locus.\nThere are short-read and long-read NSG approaches. Short reads of NGS range in size from 75 to 300 bp depending on the application and sequencing chemistry. NGS is taken to mean second generation technologies, however 3G and 4G technologies have since evolved (enable longer read sequences in excess of 10 kilobases).\nAmong 2G NGS chemistries, Illumina sequencing by synthesis (SBS) is the most widely adopted worldwide, responsible for generating more than 90% of the world‚Äôs sequencing data.\n\n\n\n\n\nWith this wealth of insights about the experiment at hand, we can now move forward to obtain the necessary equipment for analyzing this type of data effectively. üòé"
  },
  {
    "objectID": "pages/Day1.html#setting-up-rstudio",
    "href": "pages/Day1.html#setting-up-rstudio",
    "title": "Day 1",
    "section": "Setting up Rstudio",
    "text": "Setting up Rstudio\nFirst of all, let‚Äôs start assembling the tools that you will need for this workshop! While R by itself has no particular user interface, RStudio is the tool that allows you to click and actually ‚Äòsee‚Äô what you are doing. RStudio is an integrated development environment (IDE) and will be our primary tool to interact with R.\nYou can follow two strategies:\n\nInstall R and RStudio on your machine\nRun RStudio in a browser using RStudio Cloud.\n\nNote that the first option will require a bit more time and effort. To get started with RStudio Cloud (now Posit Cloud), click on this link and create a Posit free account. Once you completed your registration, you are redirected to Your Workspace. To start a new R project, you can click on New Project &gt; New RStudio Project. This will open RStudio on the Cloud."
  },
  {
    "objectID": "pages/Day1.html#the-rstudio-interface",
    "href": "pages/Day1.html#the-rstudio-interface",
    "title": "Day 1",
    "section": "The RStudio Interface",
    "text": "The RStudio Interface\nNow that you have RStudio open, you should see something similar to this:\n\n\n\nThe RStudio interface is composed of quadrants:\n\nThe Console window: located in the bottom-left, it‚Äôs where you will find the output of your coding, and it‚Äôs also possible to type R code interactively on the console.\nThe Files / Plots / Packages / Help / Viewer window: located in the bottom-right, it displays your current file system (on the Cloud), which is pointed to the position where your Rproject you just created lives.\n\nThe Files pane lists the files and folders in your root directory (i.e.¬†where by default RStudio saves your analyses).\nAnother important pane is Plots, designed to show you any plots you have created.\nThe Package pane instead allows to manage all the additional tools you can import and use when you perform your analyses. They are useful to enhance the basic R functions from different aspects.\nIf you need H E L P, you can use the Help pane: you can search for specific topics, for instance how a computation works and other kinds of documentation.\n\nThe Environment / History / Connections / Tutorial window: you can find it in the top right position.\n\nIt is pointed to the Environment pane, that is a place where you can find all the objects available for computation (e.g.¬†datasets, plots, lists, etc.).\nThe History pane keeps whatever computation you run in the console. You can re-run the computation stored in the history and you can also copy the past code into an existing or new R Script."
  },
  {
    "objectID": "pages/Day1.html#r-fundamentals",
    "href": "pages/Day1.html#r-fundamentals",
    "title": "Day 1",
    "section": "R fundamentals",
    "text": "R fundamentals\n\nBasic computations\nThe most basic computation you can do in R is arithmetic operations. Let‚Äôs take a look at the following examples:\n\n# Addition\n14 + 7\n\n[1] 21\n\n# Division\n14 / 7\n\n[1] 2\n\n# Square root\nsqrt(14)\n\n[1] 3.741657\n\n\nBut R is much more than an enormous calculator! Besides arithmetic operations, there are also logical queries you can perform. Logical queries always return either the value TRUE or FALSE.\n\n#1 Is it TRUE or FALSE?\n3 == 3\n\n[1] TRUE\n\n#2 Is 45 bigger than 55?\n50 &gt; 56\n\n[1] FALSE\n\n#3 Are these two words NOT the same?\n\"Gene\" != \"gene\"\n\n[1] TRUE\n\n\n\nNotice that in logical queries, equal to is represented by == and not by = , which instead is used to assign a value to an object. However, in practice, most R programmers tend to avoid = since it can easily lead to confusion with ==.\n\n\n\nAssigning values to objects with &lt;-\nAssigning values to objects is another common task that we will perform. What can be an object ?\n\na list of names\na series of number\na dataset\na plot\na function\netc.\n\nIn short, an object can enclose different things which form part of your data analysis. For example, objects are useful to store results that you want to process further in later steps of the analysis. We have to use the assign operator &lt;- to assign a value to an object. Let‚Äôs have a look at an example:\n\n# Let's assign the gene \"TP53\" to the \"genes\" object\ngenes &lt;- \"TP53\"\n\nNow the object genes carries the value \"TP53\". If you check out the Environment pane, you should see that the gene object appeared there. We can also print values of an object in the console by simply typing the name of the object genes and hit Return ‚Üµ.\n\n# What are my genes?\ngenes\n\n[1] \"TP53\"\n\n\nTo create objects with multiple values, we can use the function c() which stands for ‚Äòconcatenate‚Äô:\n\n# Adding some more genes to the object\ngenes &lt;- c(\"TP53\",\n           \"TNF\",\n           \"EGFR\",\n           \"VEGFA\",\n           \"APOE\",\n           \"IL6\")\n           \n# Here are my genes\ngenes\n\n[1] \"TP53\"  \"TNF\"   \"EGFR\"  \"VEGFA\" \"APOE\"  \"IL6\"  \n\n\nTo concatenate values into a single object, we need to use a comma , to separate each value. Otherwise, R will report an error back.\n\n\ngenes &lt;- c(\"TP53\" \"TNF\")\n\nError: &lt;text&gt;:1:19: unexpected string constant\n1: genes &lt;- c(\"TP53\" \"TNF\"\n                      ^\n\n\n\nDon‚Äôt worry about it! R‚Äôs error messages tend to be very useful and give meaningful clues to what went wrong. In this case, we can see that something ‚Äòunexpected‚Äô happened and it shows where our mistake is."
  },
  {
    "objectID": "pages/Day1.html#object-types-list",
    "href": "pages/Day1.html#object-types-list",
    "title": "Day 1",
    "section": "Object types: list",
    "text": "Object types: list\nValues inside objects are meant to be of the same type, for instance numeric or character. Consequently, mixing different types of data into one object is likely a bad idea. However, there is an exception: a list object can hold other objects of different data types.\n\ndata &lt;- list(genes,\n             c(1, 2, 3, 4),\n             c(\"TGFB1\", 5, \"AKT1\", 6))\ndata\n\n[[1]]\n[1] \"TP53\"  \"TNF\"   \"EGFR\"  \"VEGFA\" \"APOE\"  \"IL6\"  \n\n[[2]]\n[1] 1 2 3 4\n\n[[3]]\n[1] \"TGFB1\" \"5\"     \"AKT1\"  \"6\"    \n\n\n\nYou will notice in this example that we do not use \"\" for each value in the list. This is because genes is not a character value, but an object. When we refer to objects, we do not need quotation marks.\n\nLooking at the list item in the environment page, you can see that the object data is classified as a List of 3, and if you click on the blue icon, you can inspect the different objects inside.\n\n\n\n\nFunctions\nWe mention the term ‚Äòfunction‚Äô multiple times, but what are functions and why we need them? In simple terms, functions are objects that contain lines of codes and can be seen as shortcuts for our programming. They are useful to save space and time and to make our code more readable and reliable. We will make an example to understand better, by calculating the arithmetic mean:\n\n# We create an object that stores our values\nvalues &lt;- c(1, 5, 7, 4, 13, 2, 9, 5)\n\n# Manually compute the sum \nsum &lt;- 1 + 5 + 7 + 4 + 13 + 2 + 9 + 5\n\n# Divide the sum by the number of values\nmean &lt;- sum / 8\nmean\n\n[1] 5.75\n\n\nOr‚Ä¶\n\n# Use the mean function that is part of the R base package\nmean(values)\n\n[1] 5.75\n\n\n\n# Let's check that the two methods coincide\nsum / 8 == mean(values)\n\n[1] TRUE\n\n\n\n\nData frames\nThe last type of objects we will see (of course there are far many more types‚Ä¶) is the data.frame, one of the most abundantly used in R. Think of data.frames as the R equivalent of Excel spreadsheets, so a way to store tabular data. As we will see later, pretty much all the data we are going to handle will be in the form of a data.frame or some of its other variations.\n\n# Let's create and display a data frame (a table) with four rows and two columns\ndata.frame(\"Class\"=c(\"a\",\"b\",\"c\",\"d\"), # First column\n            \"Quantity\"=c(1,10,4,6)) # Second column\n\n  Class Quantity\n1     a        1\n2     b       10\n3     c        4\n4     d        6\n\n\nWe have instructed R to create a data.frame object. We will give more of these commands from now on!"
  },
  {
    "objectID": "pages/Day1.html#create-an-rscript",
    "href": "pages/Day1.html#create-an-rscript",
    "title": "Day 1",
    "section": "Create an Rscript",
    "text": "Create an Rscript\nYou might want to save your code for a later use, especially when it starts to become long and complex. In this case, it is not very convenient to write it in the console, which does not keep track of it (as far as you don‚Äôt go back in the History) and does not allow to edit the code and save changes.\nSo, instead, we can write code into an R Script, which is a document that RStudio recognizes as R programming code and has .R as extension. Files that are not R Scripts, like .txt, .rtf or .md, can also be opened in RStudio, but any code written in it will not be automatically recognized.\nWhen opening an R script or creating a new one, it will display in the Source window. The term ‚Äòsource‚Äô can be understood as any type of file, e.g.¬†data, programming code, notes, etc.\n\nNow let‚Äôs create an R script, by selecting File &gt; New File &gt; R Script in the menu bar, or using the keyboard shortcut Ctrl + Shift + N on PC and Cmd + Shift + N on Mac. We will name it ‚ÄúDay1.R‚Äù.\nNow you can go in the History, copy the lines previously coded (by clicking on the first line you want to copy, then pressing Shift + Down arrow, up to the last line of code) and then click To Source.\n‚ùóÔ∏è Writing some code in your R script will NOT automatically run it! If you tried pressing Return ‚Üµ, you would only add a new line. Instead, you need to select the code you want to run and press Ctrl+Return ‚Üµ (PC) or Cmd+Return ‚Üµ (Mac)."
  },
  {
    "objectID": "pages/Day1.html#install-the-packages",
    "href": "pages/Day1.html#install-the-packages",
    "title": "Day 1",
    "section": "Install the packages",
    "text": "Install the packages\nThe analyses we are going to perform require specific functions that are not included in the basic set of functions in R. These functions are collected in specific packages. R packages are extensions to the R programming language that contain code, data and documentation which help us performing standardized workflows. In the chunk below, we instruct R to install the packages that we will need later on through the workshop.\n\nCopy this chunk and paste it to your R script.\n\n\n# Install packages from Bioconductor\nif (!require(\"BiocManager\", quietly = TRUE))\n  install.packages(\"BiocManager\")\n\n# Install packages from CRAN\ninstall.packages(\"tidyr\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"googledrive\")\n\n# For differential analysis\nBiocManager::install(\"vsn\")\nBiocManager::install(\"edgeR\")\ninstall.packages(\"statmod\")\n\n# For visualizations\ninstall.packages(\"hexbin\")\ninstall.packages(\"pheatmap\")\ninstall.packages(\"RColorBrewer\")\ninstall.packages(\"ggrepel\")\ninstall.packages(\"circlize\")\n\n# For downstream analysis\ninstall.packages(\"gprofiler2\")\n\n# Clean garbage\ngc()\n\nDuring the installation, you will see many messages being displayed on your R console, don‚Äôt pay too much attention to them unless they are red and specify an error!\nIf you encounter any of these messages during installation, follow this procedure here:\n\n# R asks for package updates, answer \"n\" and type enter\n# Question displayed:\nUpdate all/some/none? [a/s/n]:\n\n# Answer to type:  \nn\n\n# R asks for installation from binary source, answer \"no\" and type enter\n# Question displayed:\nDo you want to install from sources the packages which need compilation? (Yes/no/cancel)\n\n# Answer to type:\nno\n\nWhile the packages are installed, we can start diving into the ChIP-seq core processing steps."
  },
  {
    "objectID": "pages/Day1.html#the-basics-of-chip-seq-data-processing",
    "href": "pages/Day1.html#the-basics-of-chip-seq-data-processing",
    "title": "Day 1",
    "section": "The basics of ChIP-seq data processing",
    "text": "The basics of ChIP-seq data processing\n\nRaw Sequencing Output\nThe raw output of any sequencing run consists of a series of sequences (called tags or reads). These sequences can have varying length based on the run parameters set on the sequencing platform. Nevertheless, they are made available for humans to read under a standardized file format known as FASTQ. This is the universally accepted format used to encode sequences after sequencing. An example of real FASTQ file with only two reads is provided below.\n\n@Seq1\nAGTCAGTTAAGCTGGTCCGTAGCTCTGAGGCTGACGAGTCGAGCTCGTACG\n+\nBBBEGGGGEGGGFGFGGEFGFGFGGFGGGGGGFGFGFGGGFGFGFGFGFG\n@Seq2\nTGCTAAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGC\n+\nEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n\nFASTQ files are an intermediate file in the analysis and are used to assess quality metrics for any given sequence. The quality of each base call is encoded in the line after the + following the standard Phred score system.\n\nüí° Since we now have an initial metric for each sequence, it is mandatory to conduct some standard quality control evaluation of our sequences to eventually spot technical defects in the sequencing run early on in the analysis.\n\n\n\nQuality metrics inspection\nComputational tools like FastQC aid with the visual inspection of per-sample quality metrics from NGS experiments. Some of the QC metrics of interest to consider include the ones listed below, on the left are optimal metric profiles while on the right are sub-optimal ones:\n\nPer-base Sequence Quality  This uses box plots to highlight the per-base quality along all reads in the sequencing experiment, we can notice a physiological drop in quality towards the end part of the read.\n\n\nPer-sequence Quality Scores  Here we are plotting the distribution of Phred scores across all identified sequences, we can see that the high quality experiment (left) has a peak at higher Phred scores values (34-38).\n\n\nPer-base Sequence Content  Here we check the sequence (read) base content, in a normal scenario we do not expect any dramatic variation across the full length of the read since we should see a quasi-balanced distribution of bases.\n\n\nPer-sequence GC Content  GC-content refers to the degree at which guanosine and cytosine are present within a sequence, in NGS experiments which also include PCR amplification this aspect is crucial to check since GC-poor sequences may be enriched due to their easier amplification bias. In a normal random library we would expect this to have a bell-shaped distribution such as the one on the left.\n\n\nSequence Duplication Levels  This plot shows the degree of sequence duplication levels. In a normal library (left) we expect to have low levels of duplication which can be a positive indicator of high sequencing coverage.\n\n\nAdapter Content  In NGS experiments we use adapters to create a library. Sometimes these can get sequenced accidentally and end up being part of a read. This phenomenon can be spotted here and corrected using a computational approach called adapter trimming.\n\nThe read trimming step consists of removing a variable portion of read extremities that contain adapters/have suboptimal quality indicated by the Phred score. Tools like Cutadapt can be used to perform this task after read QC, and FASTQC can be eventually run just after read trimming to double-check positive effect of getting rid of bad sequences. Note that after this step, reads might have different lengths.\n\n\nRead alignment\nNow that we have assessed the quality of the sequencing data, we are ready to align the reads to the reference genome in order to map the exact chromosomal location they derive from.\n\nA reference genome is a set of nucleic acid sequences assembled as a representative example of a species‚Äô genetic material. It does not accurately represent the set of genes of any single organism, but a mosaic of different nucleic acid sequences from each individual. For each model organism, several possible reference genomes may be available (e.g.¬†hg19 and hg38 for human).\nAs the cost of DNA sequencing falls, and new full genome sequencing technologies emerge, more genome sequences continue to be generated. New alignments are built and the reference genomes improved (fewer gaps, fixed misrepresentations in the sequence, etc). The different reference genomes correspond to the different released versions (called ‚Äúbuilds‚Äù).\n\nA mapper tool takes as input a reference genome and a set of reads. Its aim is to match each read sequence with the reference genome sequence, allowing mismatches, indels and clipping of some short fragments on the two ends of the reads.\n\nIllustration of the mapping process \n\nAmong the different mappers, Bowtie2 is a fast and open-source aligner particularly good at aligning sequencing reads of about 50 up to 1,000s of bases to relatively long genomes. By default, it performs a global end-to-end read alignment and by changing the settings, it also supports the local alignment mode. You can understand the difference by looking at this scheme:\n\n\n\nBowtie2 can identify reads that are:\n\nuniquely mapped: read pairs aligned exactly 1 time\nmulti-mapped: reads pairs aligned more than 1 time\nunmapped: read pairs non concordantly aligned or not aligned at all\n\n\nMulti-mapped reads can happen because of repetition in the reference genome (e.g.¬†multiple copies of a gene), particularly when reads are small. It is difficult to decide where these sequences come from and therefore most of the pipelines ignore them.\n\n\nMapping statistics\nChecking the mapping statistics is an important step to perform before to continue any analyses. There are several potential sources for errors in mapping, including (but not limited to):\n\nPCR artifacts: PCR errors will show as mismatches in the alignment\nsequencing errors\nerror of the mapping algorithm due to repetitive regions or other low-complexity regions.\n\nA low percentage of uniquely mapped reads is often due to (i) either excessive amplification in the PCR step, (ii) inadequate read length, or (iii) problems with the sequencing platform.\n\n70% or higher is considered a good percentage of uniquely mapped reads over all the reads, whereas 50% or lower is concerning. The percentages are not consistent across different organisms, thus the rule can be flexible!\n\nBut where the read mappings are stored?\n\n\nThe BAM file format\nA BAM (Binary Alignment Map) file is a compressed binary file storing the read sequences, whether they have been aligned to a reference sequence (e.g., a chromosome), and if so, the position on the reference sequence at which they have been aligned.\nA BAM file (or a SAM file, the non-compressed version) consists of:\n\nA header section (the lines starting with @) containing metadata, particularly the chromosome names and lengths (lines starting with the @SQ symbol)\nAn alignment section consisting of a table with 11 mandatory fields, as well as a variable number of optional fields.\n\n\nBAM file format \n\nQuestions:\n\n\nWhich information do you find in a BAM file that you also find in the FASTQ file?\n# Sequences and quality information\n\n\n\n\nWhat is the additional information compared to the FASTQ file?\n# Mapping information, Location of the read on the chromosome, Mapping quality, etc\n\n\n\n\n\nPeak calling\nThe read count data generated by ChIP-seq is massive. In order to predict the actual sites where the protein of interest is likely to bind, different peak calling methods have been developed. Peaks are regions with significant number of mapped reads that produce a pileup.\n\nChIP-seq is most often performed with single-end reads, and ChIP fragments are sequenced from their 5‚Äô ends only. This creates two distinct peaks of read density! One on each strand with the binding site falling in the middle of these peaks. The distance from the middle of the peaks to the binding site is often referred to as the ‚Äúshift‚Äù.\n\n\n\nForward (blue) and reverse (maroon) Read Density Profiles derived from the read data contribute to the Combined Density Profile (orange). Nat. Methods,2008\n\nThe most popular tool to find peaks is MACS2 which empirically models the shift size of ChIP-Seq reads, and uses it to improve the spatial resolution of predicted binding sites. Briefly, these are the steps performed by MACS:\n\nremoving duplicate reads\nmodelling the shift size\nscaling the libraries with respect to their controls\nperforming peak detection\nestimating False Discovery Rate (FDR)\n\nThe peak detection identifies areas in the genome that have been enriched with mapped reads.\n\nEnrichment = Immunoprecipitation reads/background reads (mock IP or untagged IP).\nIf an experimental control data is NOT available, a random genomic background is assumed.\n\n\n\n\n\nOverview of the Peak Calling step\n\n\n\nFinally, peaks are filtered to reduce false positives and ranked according to relative strength or statistical significance.\n\nDifferent peak profiles\nAn important factor that influences the sequencing depth that is required for a ChIP‚Äìseq experiment is whether the protein (or chromatin modification) is a point-source factor, a broad-source factor or a mixed-source factor.\n\nPoint sources occur at specific locations in the genome. This class includes sequence-specific transcription factors as well as some highly localized chromatin marks, for example, those associated with enhancers and transcription start sites. They will generate more often narrow peaks.\nBroad sources are generally those that cover extended areas of the genome, such as many chromatin marks (for example, histone H3 lysine 9 trimethylation (H3K9me3) marks). They will give raise to broad peaks.\nMixed-source factors, such as RNA polymerase II, yield both types of peaks. As expected, broad-source and mixed-source factors require a greater number of reads than point-source factors. Peaks generated will have a mixed profile between narrow and broad.\n\n\n\n\n\n\nAdditional QC metrics\nAfter peak calling, it‚Äôs important to check some metrics that are indicative of the overall quality of the ChIP-seq experiment. Here are two of the most useful:\n\nFRiP score: reports the percentage of reads overlapping within called peak. Can be useful to understand how much the IP sample is ‚Äúenriched‚Äù.\nStrand cross-correlation: high-quality ChIP-seq produces significant clustering of enriched reads at locations bound by the protein of interest, that present as a bimodal density profile on the forward and reverse strands (peaks). The cross-correlation metric calculates how many bases to shift the bimodal peaks in order to get the maximum correlation between the two read clusters, which corresponds to the predominant fragment length.\n\n\n\nBigWig and BED file formats\nMost high-throughput data can be viewed as a continuous score over the bases of the genome. In case of RNA-seq or ChIP-seq experiments, the data can be represented as read coverage values per genomic base position.\nThis sort of data can be stored as a generic text file or can have special formats such as Wig (stands for wiggle) from UCSC, or the bigWig format, which is an indexed binary format of the wig files.\n\nThe bigWig format is great for data that covers a large fraction of the genome with varying and continuous scores, because the file is much smaller than regular text files that have the same information and it can be queried more easily since it is indexed. Most of the ENCODE project data can be downloaded in bigWig format!\n\nIf we want to know instead the peak locations in the genome as chromosome coordinates with well defined start and end positions, we want to look at BED files, that are the main output of the peak calling step.\nA typical BED file is a text file format used to store genomic regions as coordinates. The data are presented in form of columns separated by tabs. This is the structure of a standard BED file, which can have also additional columns.\n\n\n\n\nWe will use this file type to inspect peak locations and for future downstream analyses!\n\n\n\n\nCollapse data into a single dataset\nImagine that you are performing these steps of read processing not just for one library, but for a collection of samples with different replicates and experimental conditions, and you subsequently want to make differential comparisons across conditions. To this aim, it‚Äôs necessary to collapse the single peak files obtained from each single library into a single consensus peakset.\nIn this step you can also add a filtering to exclude genomic intervals that are identified as peaks only in a minority of samples. DiffBind and BEDTools are two common programs that can handle this task.\nIt can be helpful to make a plot of how many peaks overlap in how many samples, like this one:\n\n\nThis plot shows that there are almost 4000 total merged peaks, representing the union of all intervals. At the other extreme, there are around 100 peaks that overlap in all 11 samples, representing the intersection of all the samples.\n\n\nWhich should we chose? Given the rigor of the statistical analysis that we are going to perform, now we can choose a more inclusive consensus set. The default is to make a consensus peakset using peaks identified in at least 2 samples.\n\n\n\nCounting reads in consensus peaks\nOnce consensus genomic regions are defined, the next step is to count the number of sequencing reads that align to these regions. This read counting process provides quantitative information about the strength and extent of protein binding within the identified peaks.\n\nUniquely mapped reads can be counted within peaks in the consensus using a tool called FeatureCounts.\n\nSample counts for all the consensus peaks are output as a tabular file, in which each row represents one peak, and you have one column for each sample. We will now load one and take a closer look: this will be our starting point in the hands-on analysis of a ChIP-seq dataset.\n\nBefore going on, let‚Äôs summarize the steps of a ChIP-seq analysis workflow that we have seen so far with the following scheme. You can find the file format names associated to the output of each task."
  },
  {
    "objectID": "pages/Day1.html#the-tested-dataset",
    "href": "pages/Day1.html#the-tested-dataset",
    "title": "Day 1",
    "section": "The tested dataset",
    "text": "The tested dataset\nThe dataset we will analyze comes from this paper published from our lab in 2021:\n\nDella Chiara, Gervasoni, Fakiola, Godano et al., 2021 - Epigenomic landscape of human colorectal cancer unveils an aberrant core of pan-cancer enhancers orchestrated by YAP/TAZ\n\nIts main object was to explore the way cancer cells use enhancers as opposed to nonmalignant cells, and we did so by using ChIP-seq on histone marks extensively. We generated organoid lines to model colorectal tumors, analyzed their epigenome, and compared it to the ATAC-seq-derived chromatin activity data of multiple other malignancies. We found a small number of enhancer sequences were conserved across all these types, and identified a transcription factor that linked them - the YAP/TAZ pair - that when perturbed would cause the death of malignant, but not normal, organoids, therefore representing a potential pan-cancer therapeutic target.\n\n\n\n‚úÖ In this workshop, we will re-analyze a portion of the ChIP-seq dataset used in the paper, pertaining to Histone 3 Lysine 27 Acetylation - a marker of activation present at active regulatory sequences, enhancers and promoters alike."
  },
  {
    "objectID": "pages/Day1.html#load-and-explore-the-dataset",
    "href": "pages/Day1.html#load-and-explore-the-dataset",
    "title": "Day 1",
    "section": "Load and explore the dataset",
    "text": "Load and explore the dataset\nIn order to speed up the computations and keep memory usage low, we have subset the dataset only to chromosome 12. It will be interesting to see if we can recapitulate similar analyses by looking only at a single chromosome ü§ì.\nData are in this public Google Drive folder. You will find:\n\nraw_counts_chr12.matrix: the peak by sample matrix containing the number of reads detected for each peak in each sample.\ncolData.txt: a tabular file containing our metadata related to columns of the count table, which contains different info about our samples (e.g.¬†the treatment, the sample origin, etc.).\npeakset.bed: a BED file with the chromosome locations of the entire consensus peakset (i.e.¬†all chromosomes).\nrecurrence_chr12.matrix: this is a table where you can find if any of the intervals that constitute the consensus peakset is also called as peak in each separate sample. Remember that when a consensus peakset is created, usually genomic intervals called ad peaks in at least 2 out of all the samples are kept in the consensus. Therefore, in the consensus you might find intervals that are called in 3 or in all the samples. It might be interesting to check how the peaks are distributed.\ndba_Korg_Ntissue_homer_annot.txt: this file contains information about the annotation of each consensus peakset to the nearest gene TSS. You will understand later its usage.\nKorg_UP_regions_results.txt and Ncr_UP_regions_results.txt: these files store the differential analysis results for the entire peakset and we will need them on day 3 to perform downstream functional analyses.\n\nOpen the folder through Google Drive, check the presence of the files in the browser and THEN run the code below.\n\nAfter you run the code below, look into your R console and check if you are prompted to insert your Google account information. Do so and then follow the instructions to connect to your Google account in order to download the data from the shared folder!\n\n\n# Load installed packages with the \"library()\" function\nlibrary(dplyr)\nlibrary(googledrive)\n\n# Load files\nfiles &lt;- drive_ls(path=\"MOBW2024_uploadata\")\n\n# File paths with URL\ncounts &lt;- files[files$name == \"raw_counts_chr12.matrix\",] %&gt;% drive_read_string() %&gt;% read.table(text = ., sep=\"\\t\") %&gt;% as.data.frame()\n\nsamples &lt;- files[files$name == \"colData.txt\",] %&gt;% drive_read_string() %&gt;% read.table(text = ., sep=\"\\t\") %&gt;% as.data.frame()\n\nWe can now explore the data that we have just loaded in the current R session to familiarize with it.\n\n# Check out the counts\nhead(counts, 10)\n\n\n\n\n\n\n\n\nSQ_2157\nSQ_1990\nSQ_2010\nSQ_2163\nSQ_2204\nSQ_2212\nSQ_2216\nSQ_2222\nSQ_2288\nSQ_2303\nSQ_2298\nSQ_2145\nGSM2058021\nGSM2058022\nGSM2058023\n\n\n\n\nreg_6364\n8\n1\n1\n97\n1\n13\n3\n12\n24\n19\n47\n4\n85\n34\n185\n\n\nreg_6365\n26\n20\n27\n47\n58\n67\n47\n52\n65\n21\n43\n11\n79\n56\n216\n\n\nreg_6366\n1\n9\n1\n6\n1\n1\n1\n7\n1\n1\n45\n1\n65\n44\n169\n\n\nreg_6367\n1\n1\n1\n21\n2\n14\n4\n5\n2\n1\n52\n29\n132\n66\n188\n\n\nreg_6368\n1\n1\n15\n17\n6\n10\n6\n1\n11\n5\n11\n3\n11\n23\n56\n\n\nreg_6369\n184\n304\n215\n661\n278\n519\n466\n435\n500\n267\n1045\n297\n1950\n1562\n3860\n\n\nreg_6370\n28\n1\n2\n92\n15\n21\n19\n50\n49\n1\n57\n9\n91\n44\n136\n\n\nreg_6371\n141\n290\n281\n673\n196\n398\n438\n303\n451\n188\n666\n222\n1793\n1409\n3416\n\n\nreg_6372\n1\n1\n3\n26\n3\n1\n1\n1\n1\n1\n39\n1\n39\n25\n222\n\n\nreg_6373\n63\n86\n112\n293\n70\n168\n55\n99\n268\n135\n187\n31\n580\n229\n894\n\n\n\n\n\n\n\n\nWe can then check the shape of our counts table (i.e.¬†how many different peaks we are detecting and how many different samples?)\n\n# How many rows and columns does our count table have?\ndim(counts)\n\n[1] 1581   15\n\n\nWe can see that our table contains count information for 1581 peaks and 15 samples.\nWe can also inspect the metadata from the samples which is stored in the samples object we created above.\n\n# What does the table look like?\nsamples\n\n\n\n\n\n\n\n\ngroups\nsizeFactor\nTissue\nSampleID\nTreatment\n\n\n\n\nSQ_2157\nK_org\n2.489936\n4KE\nSQ_2157\nK_org\n\n\nSQ_1990\nK_org\n2.758060\n8KE\nSQ_1990\nK_org\n\n\nSQ_2010\nK_org\n2.760352\n10KE\nSQ_2010\nK_org\n\n\nSQ_2163\nK_org\n2.425892\n13KE\nSQ_2163\nK_org\n\n\nSQ_2204\nK_org\n1.863534\n18KE\nSQ_2204\nK_org\n\n\nSQ_2212\nK_org\n2.580778\n11KW\nSQ_2212\nK_org\n\n\nSQ_2216\nK_org\n2.159358\n24KE\nSQ_2216\nK_org\n\n\nSQ_2222\nK_org\n2.203927\n22KE\nSQ_2222\nK_org\n\n\nSQ_2288\nK_org\n2.402964\n36KE\nSQ_2288\nK_org\n\n\nSQ_2303\nK_org\n2.430445\n41KE\nSQ_2303\nK_org\n\n\nSQ_2298\nN_crypts\n3.756529\nCR_41_mp\nSQ_2298\nN_crypts\n\n\nSQ_2145\nN_crypts\n2.804850\nCR_28_mp\nSQ_2145\nN_crypts\n\n\nGSM2058021\nN_crypts\n1.252530\nCR_28\nGSM2058021\nN_crypts\n\n\nGSM2058022\nN_crypts\n1.000000\nCR_29\nGSM2058022\nN_crypts\n\n\nGSM2058023\nN_crypts\n2.476903\nCR_37\nGSM2058023\nN_crypts\n\n\n\n\n\n\n\n\n\n# What is the shape of this samples table?\ndim(samples)\n\n[1] 15  5\n\n\nIn this case, this samples table has as many rows (15) as there are samples (which in turn is equal to the number of columns in the counts table), with columns containing different types of information related to each of the samples in the analysis."
  },
  {
    "objectID": "pages/Day1.html#savingloading-files",
    "href": "pages/Day1.html#savingloading-files",
    "title": "Day 1",
    "section": "Saving/Loading Files",
    "text": "Saving/Loading Files\nLet‚Äôs save this object with samples information in a file on this cloud session, this might be needed later if we end up in some trouble with the R session! We will save it in a file format where columns are separated by commas. You might be familiar with this format if you have worked quite a bit in Excel. In R, we can save tabular data with the write.table() function specifying the location (the file name) we want. This is useful in the case our R session dies or we decide to interrupt it. In this case we will not have to run the whole analysis from the beginning and we can just source the file and load it!\n\nwrite.table(samples, \"samples_table.csv\", sep = \",\", quote = FALSE)\n\nWe can load the object back into the current session by using the following code line:\n\nsamples &lt;- read.table(\"samples_table.csv\", sep = \",\")\n\n\nWe will also repeat this procedure with the results of the differential expression analysis in order to avoid repeating work we have already done in case of any trouble!"
  },
  {
    "objectID": "pages/index.html",
    "href": "pages/index.html",
    "title": "Workshop Introduction",
    "section": "",
    "text": "Welcome! This workshop has been created as part of the Unimi course ‚ÄúMolecular Biology applied to Biotechnology‚Äù. The aim is to introduce you to the world of bioinformatics and to some of the most important concepts related to the analysis of Next Generation Sequencing (NGS) data.\nIn particular, you will be guided through the main steps of a ChIP-seq experiment, starting from the experimental design and its major challenges and then diving into a ChIP-seq analysis workflow! The hands-on part is based on R and for this reason an essential introduction to this programming language will be provided as well.\nThe dataset used in this workshop is taken from our study published on Nature Communications on 2021, ‚ÄúEpigenomic landscape of human colorectal cancer unveils an aberrant core of pan-cancer enhancers orchestrated by YAP/TAZ‚Äù. Part of the adventure will be dedicated to trying to reproduce some relevant analyses and plots published as results!\n\nLearning Objectives\n\nGet an overview of the ChIP-seq assay and its significance in genomic research\nUnderstand applications and key insights derived from ChIP-seq experiments\nFamiliarize with basic R functionalities\nDescribe the core steps of a ChIP-seq data analysis pipeline\nLearn how to navigate and utilize public resources for the analysis and exploration of genomic data\n\n\n\nWorkshop Schedule\nThis workshop is intended as a three-day tutorial. Each day will be dedicated to specific activities:\n\nDay 1\n\nIntroduction to ChIP-seq\nSetting up R environment\nUnderstanding ChIP-seq (and NGS) data core processing\nDataset introduction and exploration\n\n\n\nDay 2\n\nData normalization with edgeR\nDiagnostic and exploratory analysis\nDifferential analysis for ChiP-seq data\nVisualization of the results\n\n\n\nDay 3\n\nDownstream analyses on interesting subset of data, including:\n\nGene ontology with gProfiler\nMotif Analysis with MEME\n\nExploration of publicly available resources for accessing genomic data\n\n\n\n\nUs\nüßëüî¨ Jacopo Arrigoni (jacopo.arrigoni@ifom.eu)\nüë©üíª Carolina Dossena (carolina.dossena@ifom.eu)\n\n\nCredits\nThis workshop was inspired by other tutorials on ChIP-seq data analysis (the Bioconductor course, the teaching material from the HBC training and the tutorial from UCR. For the R fundamentals part look at this R book for beginners. Mattia Toninelli (mattia.toninelli@ifom.eu) helped with the development of this site and some course sections. The design of the analyses and the codes have been generated together with Michaela Fakiola (michaela.fakiola@ifom.eu).\n\n\nLicense\nAll of the material in this course is under a Creative Commons Attribution license (CC BY 4.0) which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "pages/Day2.html",
    "href": "pages/Day2.html",
    "title": "Day 2",
    "section": "",
    "text": "Data normalization with edgeR\nDiagnostic and exploratory analysis\nResources for ChIP-seq and other omics data\nInspecting data with Integrative Genome Viewer"
  },
  {
    "objectID": "pages/Day2.html#objectives",
    "href": "pages/Day2.html#objectives",
    "title": "Day 2",
    "section": "",
    "text": "Data normalization with edgeR\nDiagnostic and exploratory analysis\nResources for ChIP-seq and other omics data\nInspecting data with Integrative Genome Viewer"
  },
  {
    "objectID": "pages/Day2.html#brief-recap-of-the-dataset",
    "href": "pages/Day2.html#brief-recap-of-the-dataset",
    "title": "Day 2",
    "section": "Brief recap of the dataset",
    "text": "Brief recap of the dataset\nFirst of all, let‚Äôs make a recap on the dataset that we are going to analyze.\nEach row of the counts table corresponds to a genomic interval within chromosome 12. Yesterday, we discussed about a standard analysis that requires a peak caller (such as MACS) used on the aligned reads to call peaks. While having called peaks for the sample is useful in a variety of ways, it is possible to perform a quantitative analysis without using called peaks. For example, in some cases, the regions of interest may be known in advance (such as a list known gene promoters).\nThis is actually our case, because the genomic regions that we‚Äôll analyze are the active enhancers defined by ChromHMM. Indeed, using this machine-learning tool, we have characterized all the chromatin states in cells from CRC and normal colon organoids, by combining multiple histone mark ChIP-seq profiles including H3K27 acetylation. Among the different chromatin states, we were able to identify genomic regions with active enhancer features. On these regions, we counted reads from ChIP-seq on H3K27Ac for each organoid sample, and we have already did all these steps for you to speed up the process. NB: This is a compute-intensive step that might have take too long to run in the workshop! What you can see is the table with the region by sample counts."
  },
  {
    "objectID": "pages/Day2.html#data-normalization-removing-uninteresting-differences",
    "href": "pages/Day2.html#data-normalization-removing-uninteresting-differences",
    "title": "Day 2",
    "section": "Data Normalization: removing uninteresting differences",
    "text": "Data Normalization: removing uninteresting differences\nSince we want to highlight interesting biological differences between groups of samples in our dataset, the first thing to do is to ensure that we take into account and minimize all of the uninteresting differences between samples. This is accomplished by normalizing the data, a mandatory step before any differential analysis, if we want to make counts comparable both across samples and across different peaks, without any unwanted bias.\nThe main differences in the dataset that we want to normalize for are:\n\nthe Sequencing Depth (expressed as the total amount of uniquely mapped reads in a sample): even between samples that were sequenced in the same sequencing run as part of the same experiments (but even more likely if samples have been generated in multiple batches), differences in the sequencing depth are expected. The reason why it‚Äôs important to take into account them is that these differences will reflect also at the level of the single genes: differences in sequencing depths can erroneously lead to the perception of genomic intervals having differential signals.\n\n\n\n\n\nGenomic interval length: this is a potential bias if we want to compare across group of intervals other than across samples. Indeed, the larger the genomic region, the higher the amount of reads that will be counted on it, and viceversa.\n\n\n\n\nDuring the years, many approaches to data normalization have been attempted. In the table below, we summarized three common methods that can be employed to account for differences in library size (i.e.¬†the sequencing depth) and the genomic interval length.\n\n\n\n\n\n\n\n\nNormalization Method\nAccounted Factors\nDescription\n\n\n\n\nCPM (counts per million)\nSequencing depth\nCounts scaled by total read number\n\n\nTPM (transcripts per million)\nSequencing depth and gene length\nCounts per length of transcript (kb) per million mapped reads\n\n\nFPKM/RPKM\nSequencing depth and gene length\nCounts per kilobase of exon mapped reads per million mapped reads\n\n\n\n\nNotice the term ‚Äútranscript‚Äù for TPM or ‚Äúexon‚Äù in the method descriptions. Historically, these methods have been developed for RNA-seq data, dealing with gene expression counts, but they can be extended for other sequencing technologies. See also the Normalization Discussion section.\n\nCPM, TPM and FPKM/RPKM are considered simple normalization methods, that can be useful to scale NGS data. However, there are also newer and more sophisticated approached have been developed to take into account technical variability and sample-specific biases. Two of these are the DESeq2‚Äôs median of ratios and the edgeR‚Äôs trimmed mean of M values. These are indeed more advanced statistical methods used for normalization and differential analysis in multiple sequencing data, including RNA-seq and ATAC-seq.\n\n\n\nNormalization Method\nAccounted Factors\nDescription\n\n\n\n\nDESeq2‚Äôs median of ratios1\nSequencing depth and RNA composition\nCounts are divided by a sample-specific size factor\n\n\nedgeR‚Äôs trimmed mean of M values2\nSequencing depth, RNA composition and gene length\nWeighted trimmed mean of the log ratio of expression between samples\n\n\n\nAs you can see from the table, with respect to the other methods, these ones can correct for one additional unwanted difference across libraries: the Composition Bias. As the name suggests, composition biases are formed when there are differences in the composition of sequences across libraries. Highly enriched regions consume more sequencing resources and thereby suppress the representation of other regions. Scaling by library size fails to correct for this as composition biases can still occur in libraries of the same size.\n\n\n\nOut of all these, we will use one of the more advanced ones provided in the edgeR package which will be now introduced."
  },
  {
    "objectID": "pages/Day2.html#about-the-edger-package",
    "href": "pages/Day2.html#about-the-edger-package",
    "title": "Day 2",
    "section": "About the edgeR Package",
    "text": "About the edgeR Package\n\nIntroduction\nOne of the aims of this workshop is to understand which genomic regions have a differential signal for H3K27Ac in our dataset, that is, have a difference in enhancer activty. In particular, we want to identify regions that are gaining more of this histone modification in CRC organoid samples, that might be enhancer regions with increased activity compared to normal organoids. To do so, we will compare H3K27Ac levels across samples and statistically assess and quantify differences arising between the conditions represented by our categories of samples (i.e., normal crypts organoids and CRC organoids). EdgeR is a widely used package for normalization and differential analysis on bulk sequencing data.\n\nDetailed explanations of the statistical procedures implemented in the package are available in the package‚Äôs vignette.\n\nWe will start by loading the packages installed yesterday during the Set up section.\n\n# Load the package\nlibrary(edgeR)\n\nYou can have a look at all the functions contained in the edgeR package by typing the following command:\n\n# This should open a popup window in the lower right part of the screen displaying the functions in the package\n??edgeR"
  },
  {
    "objectID": "pages/Day2.html#create-a-dgelist-object-with-counts-sample-metadata-and-design-formula",
    "href": "pages/Day2.html#create-a-dgelist-object-with-counts-sample-metadata-and-design-formula",
    "title": "Day 2",
    "section": "Create a DGEList object with counts, sample metadata and design formula",
    "text": "Create a DGEList object with counts, sample metadata and design formula\nIn order for the package to read and understand our data and correctly perform the analysis, we need to organize our data in a way that the functions of the package can handle. This new object that we are going to create is called DGEList and there is a utility function to create one starting from the ingredients we currently have, (1) a table of counts (our counts object), (2) a table with sample information (our samples object) and (3) one last thing that we need to decide in order tell the package what comparisons we value the most, this is called a design formula.\n\nBehind The Design Formula\nThe design formula should contain the name of a column of interest in our table of samples (that we can call factor) which stores the information related to the levels (or categories) we want to contrast. Let‚Äôs say that we have a dataset with two conditions (condition_1 vs condition_2) that we want to compare. The samples table will look like this, with three replicates for each of the two conditions:\n\n\n\nSample Code\nPatient\nCondition\n\n\n\n\nSA1\nPt1\nCondition_1\n\n\nSA2\nPt2\nCondition_1\n\n\nSA3\nPt3\nCondition_1\n\n\nSA4\nPt1\nCondition_2\n\n\nSA5\nPt2\nCondition_2\n\n\nSA6\nPt3\nCondition_2\n\n\n\n\nPaired Analyses\nThe optimal setting for the analysis (decided experimentally) is to have paired samples. This might be a somewhat difficult concept to grasp, but for our table above this means that every Patient contributes equally to the two categories in the Condition columns that we are interested in. In this setting, we are fully capable of exploiting the statistics behind the tools we use for differential analysis by correcting for the uninteresting differences arising between patients. This aspect greatly helps the analysis and improves the statistical validity of the results.\nRemember, this is something achieved by deciding the experimental setting beforehand! Ideally this should be done through a collaborative effort between bioinformaticians/statisticians and bench scientists!\nIf we are interested in performing a differential expression analysis comparing condition_1 versus condition_2, then our design formula should specify the Condition factor.\n\nüí° What is the column that we are interested in when specifying the design formula using in our samples table?\n\nNow that we also understand the design formula, we can create the DGEList object with the data that we loaded beforehand, but first we need to check that the columns of the counts table are in the same order of the rows of the sample table, this is important since we want to be sure that the right levels of expression are associated to the right sample.\n\nall(rownames(samples) == colnames(counts))\n\n[1] TRUE\n\n\nFurther, we will get rid of non-useful columns in our sample metadata:\n\nsamples &lt;- dplyr::select(samples, -c(sizeFactor, Treatment))\n\nAnd now we can build the object:\n\n# Create a design formula. Notice that we are creating an object called \"factor\"\nsample_group &lt;- factor(samples$groups, levels=c(\"N_crypts\", \"K_org\"))\n\ndesign &lt;- model.matrix(~ sample_group)\n\n# Create a `DGEList` object and call it dds\ndds &lt;- DGEList(counts = counts, samples = samples)\n\n# Let's save the `design` in the dds object\ndds$design &lt;- design\n\nRemeber that dds is just a list in R which can be updated with different elements.\nWe can now remove the counts table from our R environment since that information is stored in our DGEList object now. This is useful to save on memory space!\n\n# Remove original `counts` table to save memory space\nrm(counts)\ngc()\n\nGreat! You have created a DGEList object which we called dds, this contains all the information related to the counts table and the sample information table in one spot. We can have a look at the sample information table and the counts table in the dds object like so:\n\n# Look at the table with sample information\nhead(dds$samples)\n\n\n\n\n\n\n\n\ngroup\nlib.size\nnorm.factors\ngroups\nTissue\nSampleID\n\n\n\n\nSQ_2157\nK_org\n69284\n1\nK_org\n4KE\nSQ_2157\n\n\nSQ_1990\nK_org\n96388\n1\nK_org\n8KE\nSQ_1990\n\n\nSQ_2010\nK_org\n112029\n1\nK_org\n10KE\nSQ_2010\n\n\nSQ_2163\nK_org\n169859\n1\nK_org\n13KE\nSQ_2163\n\n\nSQ_2204\nK_org\n108070\n1\nK_org\n18KE\nSQ_2204\n\n\nSQ_2212\nK_org\n162626\n1\nK_org\n11KW\nSQ_2212\n\n\n\n\n\n\n\n\nWe can see that some new columns were added to the samples table present in our DGEList object when we created it (the group, lib.size, norm.factors columns)! These will be used by edgeR later on for data normalization!\nWe can also take a look at the table containing the counts, which is just another element of our DGEList object:\n\n# Look at the table with count information\nhead(dds$counts)\n\n\n\n\n\n\n\n\nSQ_2157\nSQ_1990\nSQ_2010\nSQ_2163\nSQ_2204\nSQ_2212\nSQ_2216\nSQ_2222\nSQ_2288\nSQ_2303\nSQ_2298\nSQ_2145\nGSM2058021\nGSM2058022\nGSM2058023\n\n\n\n\nreg_6364\n8\n1\n1\n97\n1\n13\n3\n12\n24\n19\n47\n4\n85\n34\n185\n\n\nreg_6365\n26\n20\n27\n47\n58\n67\n47\n52\n65\n21\n43\n11\n79\n56\n216\n\n\nreg_6366\n1\n9\n1\n6\n1\n1\n1\n7\n1\n1\n45\n1\n65\n44\n169\n\n\nreg_6367\n1\n1\n1\n21\n2\n14\n4\n5\n2\n1\n52\n29\n132\n66\n188\n\n\nreg_6368\n1\n1\n15\n17\n6\n10\n6\n1\n11\n5\n11\n3\n11\n23\n56\n\n\nreg_6369\n184\n304\n215\n661\n278\n519\n466\n435\n500\n267\n1045\n297\n1950\n1562\n3860\n\n\n\n\n\n\n\n\n\nüí° In R, list elements are accessible with the $ accessor. Our dds object is indeed a list made up of three elements, the counts table, the samples table and the design table, these are accessible using $ like we did above."
  },
  {
    "objectID": "pages/Day2.html#normalizing-count-data",
    "href": "pages/Day2.html#normalizing-count-data",
    "title": "Day 2",
    "section": "Normalizing Count Data",
    "text": "Normalizing Count Data\nAs we have discussed above, normalization is an integral step to the downstream analysis and necessary for differential comparison. In this section we will normalize our data using the calcNormFactors function of the package. As we have previously introduced, edgeR uses the trimmed mean of M-values (TMM) method to calculate a set of size factors to minimize the log-fold change differences occurring between samples (uninteresting) for the majority of genomic intervals. The counts for each sample get then multiplied by the scaling factors to generate what is referred to as effective library size, which will be used for all downstream analyses.\n\n# Call the function to normalize count data\ndds &lt;- calcNormFactors(dds)\n\nWe can check the values of the computed size factors by doing the following, note how there are as many size factors as there are samples and they are inserted in a column of the samples table named norm.factors in our DGEList object:\n\ndds$samples\n\n\n\n\n\n\n\n\ngroup\nlib.size\nnorm.factors\ngroups\nTissue\nSampleID\n\n\n\n\nSQ_2157\nK_org\n69284\n0.8333710\nK_org\n4KE\nSQ_2157\n\n\nSQ_1990\nK_org\n96388\n0.9440527\nK_org\n8KE\nSQ_1990\n\n\nSQ_2010\nK_org\n112029\n1.0442464\nK_org\n10KE\nSQ_2010\n\n\nSQ_2163\nK_org\n169859\n1.0626712\nK_org\n13KE\nSQ_2163\n\n\nSQ_2204\nK_org\n108070\n0.9248773\nK_org\n18KE\nSQ_2204\n\n\nSQ_2212\nK_org\n162626\n0.8631713\nK_org\n11KW\nSQ_2212\n\n\nSQ_2216\nK_org\n166803\n0.8058559\nK_org\n24KE\nSQ_2216\n\n\nSQ_2222\nK_org\n126541\n1.0226951\nK_org\n22KE\nSQ_2222\n\n\nSQ_2288\nK_org\n194224\n1.1369122\nK_org\n36KE\nSQ_2288\n\n\nSQ_2303\nK_org\n92047\n0.8559558\nK_org\n41KE\nSQ_2303\n\n\nSQ_2298\nN_crypts\n106228\n1.4064150\nN_crypts\nCR_41_mp\nSQ_2298\n\n\nSQ_2145\nN_crypts\n31407\n1.3095585\nN_crypts\nCR_28_mp\nSQ_2145\n\n\nGSM2058021\nN_crypts\n176893\n0.9919079\nN_crypts\nCR_28\nGSM2058021\n\n\nGSM2058022\nN_crypts\n119979\n1.0117534\nN_crypts\nCR_29\nGSM2058022\n\n\nGSM2058023\nN_crypts\n287106\n0.9678679\nN_crypts\nCR_37\nGSM2058023\n\n\n\n\n\n\n\n\n\nüí° NOTE: Although edgeR does not use normalized counts as input for the differential analysis (the normalization process happens inside automatically), the normalized counts we just generated are definitely useful when plotting results and performing clustering.\n\n\nNormalization discussion\nNormalization of experimental data is particularly important in ChIP-seq (and ATAC-seq) analysis, and may require more careful consideration than needed for RNA-seq analysis. This is because the range of ChIP-seq experiments covers more cases than RNA-seq, which usually involve a similar set of possible expressed genes and/or transcripts, many of which are not expected to significantly change expression. ChIP, ATAC, and similar enrichment-based sequencing data may not follow the assumptions inherent in popular methods for normalizing RNA-seq data, as well as exhibiting different types of efficiency and other biases.\n\nReferene reads\nOne important concept is that for the type of data we are dealing with in DNA enrichment assays, the reads that are used as the reference for normalizing are as important as (or even more than) the normalization method itself. While in RNA-seq experiments the expression matrix can be normalized directly, based on the reads that uniquely overlap genes or transcripts, this does not apply to a count matrix based on a consensus peakset.\nThe DiffBind package is a tool that has been designed specifically for ChIP-seq data analysis. It is interesting to notice that using this tools, you can normalize your data using two different set of reference reads: - one uses all the reads (the full library size) - the other normalizes bases only on the total number of reads in the consensus peakset (namely, the effective library size). The major difference is that the full library size takes into account also ‚Äúbackground‚Äù reads, that are most of the reads in a ChIP-seq sample library! In brief, the fact that we consider the background might reflect in a less biased normalization calculation.\nTake home message: &gt; In this workshop, to make things less complex, we used a ‚Äúcommon‚Äù normalization method without any prior in-depth analysis on the best strategy to normalize these specific data in this context. But just keep in mind that some normalization methods might be more ‚Äúcorrect‚Äù than others in some circumstances."
  },
  {
    "objectID": "pages/Day2.html#transforming-count-data-for-visualization-purposes",
    "href": "pages/Day2.html#transforming-count-data-for-visualization-purposes",
    "title": "Day 2",
    "section": "Transforming Count Data for visualization purposes",
    "text": "Transforming Count Data for visualization purposes\nAfter we have normalized our data, we need to perform a transformation. There are many ways to transform count data but all of them achieve the goal of removing the dependence between variance and mean counts across samples (something called homoscedasticity) in order to highlight interesting and biologically relevant expression trends even for genes expressed at lower values. We transform the data using a function provided in the edgeR package called cpm() which also performs a logarithmic transformation which has the effect of reshaping the data to achieve gene-wise distributions which resemble a normal distribution. Without getting too much into the details of the workings of the function, we will transform the data and then look at how the gene-wise relationship between the mean and variance in our normalized data changes before and after the transformation. The purpose of this procedure is to allow proper data visualization later in the analysis, the transformed data is NOT used for the differential expression analysis which instead starts from raw counts!\nThe following code is used to plot the mean/standard deviation relationship of every gene before the transformation.\n\nlibrary(vsn)\n\n# Plot before data transformation\nmeanSdPlot(dds$counts)\n\n\n\n\n\n\n\n\nTransform the data and then plot the mean/standard deviation relationship after the transformation.\n\n# Transform the data with a log2 transform (watch how we create a new variable for it)\nlog2dds &lt;- cpm(dds, log=TRUE)\n\n\n# Check out the transformed values (notice how we now have floating point values and some are even negative!)\nhead(log2dds)\n\n\n\n\n\n\n\n\nSQ_2157\nSQ_1990\nSQ_2010\nSQ_2163\nSQ_2204\nSQ_2212\nSQ_2216\nSQ_2222\nSQ_2288\nSQ_2303\nSQ_2298\nSQ_2145\nGSM2058021\nGSM2058022\nGSM2058023\n\n\n\n\nreg_6364\n7.261491\n4.693504\n4.550522\n9.109187\n4.637530\n6.748099\n5.217372\n6.749666\n6.949194\n8.000180\n8.363990\n6.809124\n8.963787\n8.204418\n9.410705\n\n\nreg_6365\n8.861623\n7.874496\n7.940615\n8.104651\n9.217110\n8.943007\n8.509886\n8.702827\n8.272585\n8.136559\n8.241717\n8.141225\n8.861440\n8.895419\n9.629675\n\n\nreg_6366\n5.009203\n6.830221\n4.550522\n5.588723\n4.637530\n4.460066\n4.480605\n6.108000\n4.279070\n4.785443\n8.304149\n5.292741\n8.589938\n8.559744\n9.283187\n\n\nreg_6367\n5.009203\n4.693504\n4.550522\n7.035881\n5.124956\n6.840672\n5.480357\n5.742032\n4.581540\n4.785443\n8.503560\n9.491770\n9.583393\n9.125609\n9.433409\n\n\nreg_6368\n5.009203\n4.693504\n7.160908\n6.769029\n6.227152\n6.428312\n5.895332\n4.499083\n6.015691\n6.291778\n6.467791\n6.456553\n6.277538\n7.674925\n7.757593\n\n\nreg_6369\n11.644546\n11.712362\n10.855382\n11.844212\n11.449227\n11.857996\n11.765514\n11.721151\n11.154281\n11.732854\n12.775053\n12.820937\n13.441920\n13.653092\n13.763350\n\n\n\n\n\n\n\n\n\n# let's plot the transformed values\nmeanSdPlot(log2dds)\n\n\n\n\n\n\n\n\n\nIt is clear how regions with high mean signal (on the right) are now comparable in terms of standard deviation to regions with lower mean signal (on the left). Normally this plot should look slightly different in terms of density, but remember that we are considering only a subset of the dataset."
  },
  {
    "objectID": "pages/Day2.html#assessing-sample-to-sample-relationships",
    "href": "pages/Day2.html#assessing-sample-to-sample-relationships",
    "title": "Day 2",
    "section": "Assessing Sample-to-sample Relationships",
    "text": "Assessing Sample-to-sample Relationships\nOne way to understand trends in our data and the present of poor quality or outlier samples is to perform exploratory analyses through visualization.\nOf particular interest is the presence of technical effects in the experiment, such as batch effects.\nIn R in general, data visualization is aided by the presence of many packages (on top the basic plotting functionality) which can handle diverse kinds of data visualization tasks (from traditional plots to visualizing tabular data through heatmaps). We will encounter two of these packages, one is ggplot2 and the other one is pheatmap.\n\nHierarchical clustering\nOne of the main strategies for checking the consistency of our dataset is to cluster samples based on their complete H3K27 acetylation profile (which is considered within 1581 genomic regions in our dataset). This will allow us to spot the presence of outliers in the data and look for consistent profiles of H3K27Ac across biological replicates, which we expect. Use the code below to plot a heatmap of normalized (and transformed) count values for our samples. Since plotting the full count table can be computationally expensive, we might want to subset it to the 200 genomic regions with stronger H3K27Ac signal in the dataset.\n\nlibrary(\"pheatmap\")\n\n# Take the top 200 genomic regions in the dataset\nselect &lt;- order(rowMeans(log2dds),\n                decreasing=TRUE)[1:200] # Select number of regions\n\n# Create another table for annotating the heatmap with colors\ndf &lt;- as.data.frame(samples[,c(\"groups\",\"Tissue\")])\n\n# Draw the heatmap using the `pheatmap` package\npheatmap(log2dds[select,], cluster_rows=FALSE, show_rownames=FALSE,\n         cluster_cols=TRUE, annotation_col=df)\n\n\n\n\n\n\n\n\n\nWhat type of assessment would you make about the consistency of the samples across these top active 200 regions ? Do they cluster (a synonym for similar) based on the biological condition of our interest?\n\n\n\nSample-to-sample Distances\nAnother way to get a sense of the global relationship between samples is to check for how distant samples are between themselves. This analysis of pairwise distances looks at the acetylation signal of all 1581 genomic regions in the dataset and determines which samples have a more or less similar or different signal value for each. We expect biologically similar samples to have very little difference.\n\nlibrary(RColorBrewer)\n\n# Compute distances\nsampleDists &lt;- dist(t(log2dds))\n\n# Organize\nsampleDistMatrix &lt;- as.matrix(sampleDists)\n\ncolors &lt;- colorRampPalette( rev(brewer.pal(9, \"Blues\")) )(255)\n\n# Plot with `pheatmap`\npheatmap(sampleDistMatrix,\n         clustering_distance_rows=sampleDists,\n         clustering_distance_cols=sampleDists,\n         color = colors,\n         annotation_col = df)\n\n\n\n\n\n\n\n\n\n# Free up memory space\nrm(sampleDistMatrix)\ngc()\n\n          used (Mb) gc trigger  (Mb) limit (Mb) max used  (Mb)\nNcells 1754272 93.7    3398981 181.6         NA  2386164 127.5\nVcells 3453504 26.4    8388608  64.0      16384  5777972  44.1\n\n\n\nüí° What type of assessment would you make about the heatmap you just produced? Is the analysis highlighting biologically relevant differences in the dataset?\n\n\n\nDimensionality reduction with Principal Component Analysis (PCA)\nAnother useful approach for understanding the main variability axes in our data is to compute and plot a PCA. Without getting into the details, PCA takes our H3K27Ac data and outputs its principal components, which encode the main sources of variability in the data. Ideally, we want the samples to have variability caused by the biological effect of our interest (in this case the differences between normal and tumor organoids), but this might not be the case. By plotting and coloring the points by different covariates (i.e.¬†subject or condition) we are able to understand where the variability comes from and if there is any detectable batch effect. Use the code below to generate a scatter plot of PCA coordinates and color them to understand what causes the variability in the data.\n\nlibrary(ggplot2)\n\n# Calculate principal components and percentage of variance\npcs &lt;- prcomp(log2dds, scale = TRUE)\npercentVar &lt;- round(100 * summary(pcs)$importance[2,])\npcaData &lt;- as.data.frame(pcs$rotation) %&gt;% merge(samples, by=0)\n\n# Plot (this time with ggplot2!!)\nggplot(pcaData, aes(PC1, PC2, color=groups)) +\n  geom_point(size=3) +\n  xlab(paste0(\"PC1: \",percentVar[1],\"% variance\")) +\n  ylab(paste0(\"PC2: \",percentVar[2],\"% variance\")) + \n  theme_linedraw()+\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\n\n\nüí° What can you say about this PCA? Are the samples positioning themselves based on their biological condition? How does this information relate to the other plots we produced above?\n\n\n# Let's clean some space up!\nrm(pcs)\nrm(pcaData)\nrm(log2dds)\ngc()\n\nNow that we have plotted all the main diagnostic information related to the dataset and we have a bit of a grasp of it, we can start thinking about testing for differentially active enhancer regions."
  },
  {
    "objectID": "pages/Day2.html#online-resources-and-databases-for-omics-data",
    "href": "pages/Day2.html#online-resources-and-databases-for-omics-data",
    "title": "Day 2",
    "section": "Online Resources and Databases for omics data",
    "text": "Online Resources and Databases for omics data\nThe seciton below briefly introduce you to the description and usage of some very useful online resources for ChIP-seq public data and also other omics data. Take a closer look to explore them, you might need them in the future in order to complement your future experiments!\n\nPoor man‚Äôs guide to omics\nWhen approaching a new problem, you might conceive multiple hypotheses that could instruct a wide variety of experimental approaches. However, you have neither the time, nor the money or the inclination to perform all of these experiments. Chances are, though, that somewhere in a server there is a dataset that can help you, be it a ChIP-seq dataset or any other omic. They may not give you an answer outright, but they‚Äôll help you prune your hypotheses and only keep the ones that have some foundation. After all, while big data analysis often results in false positives, or at least positives that are contingent to a specific model, it is very useful to avoid pursuing roads for which no evidence is found in datasets.\nThis section aims to give you resources that you can employ to perform simple analyses that prevent you from wasting your time and allow you to focus on the experiments that matter.\nOver the years, multiple consortia have assembled these datasets into curated collections, and new datasets are published with almost every new paper, neatly stored into various sites.\nHere we provide a handful of key databases and characteristics of the main ones, as well as a list of other databases that may be useful for more specific questions.\n\nENCODE: the encyclopedia of DNA elements\nThe closest to the object of the workshop, over more than 20 years of activity the ENCODE consortium is the biggest effort in functional genomics in the world, assembling thousands of datasets. You can easily browse their portal by theme and find ready-made datasets that pertain to your field. Once you have your dataset (or datasets) of choice, you only have to browse it with the help of this workshop‚Äôs scripts or visualize it, for example with IGV.\nEncode project\n\n\nGEO: The Gene Expression Omnibus\nThis portal is almost ubiquitous in publications - in fact, nearly all gene expression datasets used in publications, both bulk and single cell, are published on the GEO, a service handled by the NCBI. The GEO datasets can be accessed from papers, but also directly from the portal, and the neat thing is that they are generally well annotated so that you can understand how specific experiments were performed. They have specific files that tell you how the data was generated. This kind of information can be complementary to epigenome data, or can inform derivative work giving you invaluable insight into a model you want to adopt.\nGene Expression Omnibus\n\n\nTCGA: The Cancer Genome Atlas\nAmong the biggest efforts in contemporary medical research, spearheaded by the NIH. The Cancer Genome Atlas has gathered transcriptomics, methylation, and chromatin accessibility data for an incredibly wide array of malignancies, and it keeps expanding with new cohorts of patients, curated by individual contributors. While it is a specific resource pertaining to oncology, its wealth of controls and diversity of malignancies makes it invaluable for anybody working on human biology - after all, most fundamental biology experiments are performed on cancer cell lines.\nThe Cancer Genome Atlas\n\n\nThe Human Cell Atlas\nArguably the biggest ongoing sequencing project in the world, the Human Cell Atlas aims to fully leverage single cell transcriptomes to define every cell type in the human body. Currently hosting data for close to 60 million sequenced cells and still counting, this is the largest single-cell resolved gene expression resource currently available for non-diseased tissue. Allows downloads in formats that are ‚Äòeasily‚Äô browsed - meaning you don‚Äôt have to do core processing yourself.\nHCA\n\n\nAdditional links:\nRoadmap consortium Generated tracks for multiple modifications, and produced ChromHMM to sort them out\nEBI Bioimage Archive European resource for bioimages. Pretty unique and specific, rare type of datasets.\nGTEX Tissue-specific genotype regulation - a complex database that can tell you whether your gene of interest works in the context you study.\nBrown university collection - way better than ours! Collection of resources, including some of those we showed above. Take a look when you need to find some new source of data!\nNIH‚Äôs collection - even better than Brown‚Äôs! Includes even more well-categorized resources."
  }
]