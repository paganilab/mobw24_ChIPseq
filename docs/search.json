[
  {
    "objectID": "pages/Day1.html",
    "href": "pages/Day1.html",
    "title": "Day 1",
    "section": "",
    "text": "Introduction to ChIP-seq\nSetting up R environment\nUnderstanding ChIP-seq (and NGS) data core processing\nDataset introduction and exploration\n\n\n\nChromatin immunoprecipitation (ChIP) assays are used to analyze protein interactions with DNA. By combining this method with Next Generation Sequencing, ChIP-sequencing (ChIP-seq) is a powerful method for identifying genome-wide the chromosomal locations of different kinds of proteins, including transcription factors, nucleosomes, chromatin remodeling enzymes and polymerases, and also histone modifications.\n\nChIP-seq use in mapping histone modifications has been seminal in epigenetics research! (Ku et al., 2011)\n\nThe standard ChIP-seq protocol first cross-link bound proteins to chromatin, fragments the chromatin, captures the DNA fragments bound to one protein using an antibody specific to it, and sequences the ends of the captured fragments using NGS. Here you can see the major steps:\n+ Cross-linking: add Formaldehyde\n+ Hydrolysis (breakdown of cell memberane)\n+ Lyses of DNA fragments (shearing), e.g. treat with specific endonuclease enzymes (dpn1)\n+ Add specific antibody (with beads) against the protein of interest. This will drag the protein down by immuno-precipitation\n+ Separate DNA from the protein (unlinking), and purify the DNA\n+ DNA fragments are amplified and fluorescently tagged, library construction, and fragments are then sequenced\n\n\n\n\n\n\n\n\nA common goal in ChIP-seq experiments is to identify changes in protein binding patterns between conditions (i.e. differential binding).\nEffective analysis of ChIP-seq data requires sufficient coverage by sequence reads (sequencing depth). It mainly depends on the size of the genome, and the number and size of the binding sites of the protein.\n\nFor mammalian transcription factors (TFs) and chromatin modifications such as enhancer-associated histone marks: 20 million reads are adequate\nProteins with more binding sites (e.g., RNA Pol II) or broader factors: need more reads, up to 60 million for mammalian ChIP-seq\nSequencing depth rules of thumb: &gt;10M reads for narrow peaks, &gt;20M for broad peaks\nLong & paired-end reads useful but not essential\nReplicates: The good news here is that, unlike RNA-Seq, more than 2 replicates does not significantly increase the number of targets\n\nA successful ChIP-seq experiment must have its right controls: they are essential to eliminate false positives as there is always some signal on open chromatin. Two types of controls are often used:\n\nAn “input” DNA sample (fragmented genomic DNA), the one that has been cross-linked and sonicated but not immuno-precipitated\nAn IgG “mock”-ChIP, using an antibody that will not bind to nuclear proteins and should generate random immuno-precipitated DNA.\n\n\nInput DNA control is ideal in most of the cases. Indeed, one problem with IgG control is that if too little DNA is recovered after immuno-precipitation, sequencing library will be of low complexity and binding sites identified using this control could be biased. Read this biostars post for discussion.\n\n\n\n\nNGS technologies (Illumina/PacBio) allow the processing of millions of reactions in parallel, resulting in high-throughput, higher sensitivity, speed and reduced cost compared to first generation sequencing technologies (e.g., Sanger method).\n\nGiven the vast amounts of quantitative sequencing data generated, NGS-based methods rely on resource-intensive data processing pipelines to analyze data.\n\nIn addition to the sequence itself, and unlike Sanger sequencing, the high-throughput nature of NGS provides quantitative information (depth of coverage) due to the high level of sequence redundancy at a locus.\nThere are short-read and long-read NSG approaches. Short reads of NGS range in size from 75 to 300 bp depending on the application and sequencing chemistry. NGS is taken to mean second generation technologies, however 3G and 4G technologies have since evolved (enable longer read sequences in excess of 10 kilobases).\nAmong 2G NGS chemistries, Illumina sequencing by synthesis (SBS) is the most widely adopted worldwide, responsible for generating more than 90% of the world’s sequencing data.\n\n\n\n\n\n\n\nFirst of all, let’s start assembling the tools that you will need for this workshop! While R by itself has no particular user interface, RStudio is the tool that allows you to click and actually ‘see’ what you are doing. RStudio is an integrated development environment (IDE) and will be our primary tool to interact with R.\nYou can follow two strategies:\n\nInstall R and RStudio on your machine\nRun RStudio in a browser using RStudio Cloud.\n\nNote that the first option will require a bit more time and effort. To get started with RStudio Cloud (noew Posit Cloud), click on this link and create a Posit free account. Once you completed your registration, you are redirected to Your Workspace. To start a new R project, you can click on New Project &gt; New RStudio Project. This will open RStudio on the Cloud.\n\n\n\nNow that you have RStudio open, you should see something similar to this:\n\n\n\nThe RStudio interface is composed of quadrants:\n\nThe Console window: located in the bottom-left, it’s where you will find the output of your coding, and it’s also possible to type R code interactively on the console.\nThe Files / Plots / Packages / Help / Viewer window: located in the bototm-right, it currently displays your current file system (on the Cloud), which is pointed to the position where your Rproject you just created lives.\n\nThe Files pane lists the files and folders in your root directory (i.e. where by default RStudio save your analyses).\nAnother important pane is Plots, designed to show you any plots you have created.\nThe Package pane instead allows to manage all the additional tools you can import and use when you perform your analyses. They are useful to enhance the basic R functions from different aspects.\nIf you need H E L P, you can use the Help pane: you can search for specific topics, for instance how a computation works and other kinds of documentation.\n\nThe Environment / History / Connections / Tutorial window: you can find it in the top right.\n\nIt is pointed to the Environment pane, that is a place where you can find all the objects available for computation (e.g. datasets, plots, lists, etc.).\nThe History pane keeps whatever computation you run in the console. You can rerun the computation stored in the history and you can also copy the past code into an existing or new R Script.\n\n\n\n\n\n\n\nThe most basic computation you can do in R is arithmetic operations. Let’s take a look at the following examples:\n\n# Addition\n14 + 7\n\n[1] 21\n\n# Division\n14 / 7\n\n[1] 2\n\n# Square root\nsqrt(14)\n\n[1] 3.741657\n\n\nBut R is much more than an enormous calculator! Besides arithmetic operations, there are also logical queries you can perform. Logical queries always return either the value TRUE or FALSE.\n\n#1 Is it TRUE or FALSE?\n3 == 3\n\n[1] TRUE\n\n#2 Is 45 bigger than 55?\n50 &gt; 56\n\n[1] FALSE\n\n#3 Are these two words NOT the same?\n\"Gene\" != \"gene\"\n\n[1] TRUE\n\n\n\nNotice that in logical queries, equal to is represented by == and not by = , which instead is used to assign a value to an object. However, in practice, most R programmers tend to avoid = since it can easily lead to confusion with ==.\n\n\n\n\nAssigning values to objects is another common task that we will perform. What can be an object ?\n\na list of names\na series of number\na dataset\na plot\na function\netc.\n\nIn short, an object can enclose different things which form part of your data analysis. For example, objects are useful to store results that you want to process further in later steps of the analysis. We have to use the assign operator &lt;- to assign a value to an object. Let’s have a look at an example.\n\n# Let's assign the gene \"TP53\" to the \"genes\" object\ngenes &lt;- \"TP53\"\n\nNow the object genes carries the value \"TP53\". If you check out the Environment pane, you should see that the gene object appeared there. We can also print values of an object in the console by simply typing the name of the object genes and hit Return ↵.\n\n# What are my genes?\ngenes\n\n[1] \"TP53\"\n\n\nTo create objects with multiple values, we can use the function c() which stands for ‘concatenate’:\n\n# Adding some more genes to the object\ngenes &lt;- c(\"TP53\",\n           \"TNF\",\n           \"EGFR\",\n           \"VEGFA\",\n           \"APOE\",\n           \"IL6\")\n           \n# Here are my genes\ngenes\n\n[1] \"TP53\"  \"TNF\"   \"EGFR\"  \"VEGFA\" \"APOE\"  \"IL6\"  \n\n\nTo concatenate values into a single object, we need to use a comma , to separate each value. Otherwise, R will report an error back.\n\n\ngenes &lt;- c(\"TP53\" \"TNF\")\n\nError: &lt;text&gt;:1:19: unexpected string constant\n1: genes &lt;- c(\"TP53\" \"TNF\"\n                      ^\n\n\n\nDon’t worry about it! R’s error messages tend to be very useful and give meaningful clues to what went wrong. In this case, we can see that something ‘unexpected’ happened, and it shows where our mistake is.\n\n\n\n\nValues inside objects are meant to be of the same type, for instance numeric or character. Consequently, mixing different types of data into one object is likely a bad idea. However, there is an exception: a list object can hold other objects of different data types.\n\ndata &lt;- list(genes,\n             c(1, 2, 3, 4),\n             c(\"TGFB1\", 5, \"AKT1\", 6))\ndata\n\n[[1]]\n[1] \"TP53\"  \"TNF\"   \"EGFR\"  \"VEGFA\" \"APOE\"  \"IL6\"  \n\n[[2]]\n[1] 1 2 3 4\n\n[[3]]\n[1] \"TGFB1\" \"5\"     \"AKT1\"  \"6\"    \n\n\nYou will notice in this example that I do not use \"\" for each value in the list. This is because genes is not a character value, but an object. When we refer to objects, we do not need quotation marks.\nLooking at the list item in the environment page, you can see that the object data is classified as a List of 3, and if you click on the blue icon, you can inspect the different objects inside.\n\n\n\n\n\nWe mention the term ‘function’ multiple times, but what are functions and why we need them? In simple terms, functions are objects that contain lines of codes and can be seen as shortcuts for our programming. They are useful to save space and time and to make our code more readable and reliable. We will make an example to understand better, by calculating the arithmetic mean:\n\n# We create an object that stores our values\nvalues &lt;- c(1, 5, 7, 4, 13, 2, 9, 5)\n\n# Manually compute the sum \nsum &lt;- 1 + 5 + 7 + 4 + 13 + 2 + 9 + 5\n\n# Divide the sum by the number of values\nmean &lt;- sum / 8\nmean\n\n[1] 5.75\n\n\nOr…\n\n# Use the mean function that is part of the R base package\nmean(values)\n\n[1] 5.75\n\n\n\n# Let's check that the two methods coincide\nsum / 8 == mean(values)\n\n[1] TRUE\n\n\n\n\n\nThe last type of objects we will see (of course there are far more types…) is the data.frame, one of the most abundantly used in R. Think of data.frames as the R equivalent of Excel spreadsheets, so a way to store tabular data. As we will see later, pretty much all the data we are going to handle will be in the form of a data.frame or some of its other variations.\n\n# Let's create and display a data frame (a table) with four rows and two columns\ndata.frame(\"Class\"=c(\"a\",\"b\",\"c\",\"d\"), # First column\n            \"Quantity\"=c(1,10,4,6)) # Second column\n\n  Class Quantity\n1     a        1\n2     b       10\n3     c        4\n4     d        6\n\n\n\n\n\n\nYou might want to save your code for a later use, especially when it starts to become long and complex. In this case, it is not very convenient to write it in the console, which does not keep track of it (as far as you don’t go back in the History) and does not allow to edit the code and save changes.\nSo, instead, we can write code into an R Script, which is a document that RStudio recognises as R programming code and has .R as extension. Files that are not R Scripts, like .txt, .rtf or .md, can also be opened in RStudio, but any code written in it will not be automatically recognized.\nWhen opening an R script or creating a new one, it will display in the Source window. The term ‘source’ can be understood as any type of file, e.g. data, programming code, notes, etc.\n\nNow let’s create an R script, by selecting File &gt; New File &gt; R Script in the menu bar, or using the keyboard shortcut Ctrl + Shift + N on PC and Cmd + Shift + N on Mac. We will name it Day1.R.\nNow you can go in the History, copy the lines previously coded (by clicking on the first line you want to copy, then press Shift + Down arrow, up to the last line of code and then clicking To Source.\n❗️ Writing some code in your R script will NOT automatically run it! If you tried pressing Return ↵, you would only add a new line. Instead, you need to select the code you want to run and press Ctrl+Return ↵ (PC) or Cmd+Return ↵ (Mac).\n\n\n\n\nThe analyses we are going to perform require specific functions that are not included the basic set of functions in R. These functions are collected in specific packages. R packages are extensions to the R programming language that contain code, data and documentation which help us perform standardized workflows. In the chunk below, we instruct R to install the packages that we will need later on through the workshop.\n\nCopy this chunk and paste it to your R script.\n\n\n# Install packages from Bioconductor\nif (!require(\"BiocManager\", quietly = TRUE))\n  install.packages(\"BiocManager\")\n\n# Install packages from CRAN\ninstall.packages(\"tidyr\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"googledrive\")\n\n# For differential analysis\nBiocManager::install(\"vsn\")\nBiocManager::install(\"edgeR\")\ninstall.packages(\"statmod\")\n\n# For visualizations\ninstall.packages(\"hexbin\")\ninstall.packages(\"pheatmap\")\ninstall.packages(\"RColorBrewer\")\ninstall.packages(\"ggrepel\")\ninstall.packages(\"circlize\")\n\n# For downstream analysis\ninstall.packages(\"gprofiler2\")\n\n# Clean garbage\ngc()\n\nDuring the installation, you will see many messages being displayed on your R console, don’t pay too much attention to them unless they are red and specify an error!\nIf you encounter any of these messages during installation, follow this procedure here:\n\n# R asks for package updates, answer \"n\" and type enter\n# Question displayed:\nUpdate all/some/none? [a/s/n]:\n\n# Answer to type:  \nn\n\n# R asks for installation from binary source, answer \"no\" and type enter\n# Question displayed:\nDo you want to install from sources the packages which need compilation? (Yes/no/cancel)\n\n# Answer to type:\nno\n\nWhile the packages are installed, we can start diving into the ChIP-seq core processing steps!\n\n\n\n\n\nThe raw output of any sequencing run consists of a series of sequences (called reads). These sequences can have varying length based on the run parameters set on the sequencing platform. Nevertheless, they are made available for humans to read under a standardized file format known as FASTQ. This is the universally accepted format used to encode sequences after sequencing. An example of real FASTQ file with only two reads is provided below.\n\n@Seq1\nAGTCAGTTAAGCTGGTCCGTAGCTCTGAGGCTGACGAGTCGAGCTCGTACG\n+\nBBBEGGGGEGGGFGFGGEFGFGFGGFGGGGGGFGFGFGGGFGFGFGFGFG\n@Seq2\nTGCTAAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGC\n+\nEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n\nFASTQ files are an intermediate file in the analysis and are used to assess quality metrics for any given sequence. The quality of each base call is encoded in the line after the + following the standard Phred score system.\n\n💡 Since we now have an initial metric for each sequence, it is mandatory to conduct some standard quality control evaluation of our sequences to eventually spot technical defects in the sequencing run early on in the analysis.\n\n\n\n\n\nComputational tools like FastQC aid with the visual inspection of per-sample quality metrics from NGS experiments. Some of the QC metrics of interest to consider include the ones listed below, on the left are optimal metric profiles while on the right are sub-optimal ones:\n\nPer-base Sequence Quality  This uses box plots to highlight the per-base quality along all reads in the sequencing experiment, we can notice a physiological drop in quality towards the end part of the read.\n\n\nPer-sequence Quality Scores  Here we are plotting the distribution of Phred scores across all identified sequences, we can see that the high quality experiment (left) has a peak at higher Phred scores values (34-38).\n\n\nPer-base Sequence Content  Here we check the sequence (read) base content, in a normal scenario we do not expect any dramatic variation across the full length of the read since we should see a quasi-balanced distribution of bases.\n\n\nPer-sequence GC Content  GC-content referes to the degree at which guanosine and cytosine are present within a sequence, in NGS experiments which also include PCR amplification this aspect is crucial to check since GC-poor sequences may be enriched due to their easier amplification bias. In a normal random library we would expect this to have a bell-shaped distribution such as the one on the left.\n\n\nSequence Duplication Levels  This plot shows the degree of sequence duplication levels. In a normal library (left) we expect to have low levels of duplication which can be a positive indicator of high sequencing coverage.\n\n\nAdapter Content  In NGS experiments we use adapters to create a library. Sometimes these can get sequenced accidentally and end up being part of a read. This phenomenon can be spotted here and corrected using a computational approach called adapter trimming.\n\n\nLibrary complexity is also a common quality measure for ChIP-seq libraries. It is linked to many factors such as antibody quality, over-crosslinking, amount of material, sonication, or over-amplification by PCR.\n\n\n\n\nNow that we have assessed the quality of the sequencing data, we are ready to align the reads to the reference genome.\n\nWhat is a reference genome? A set of nucleic acid sequences assembled as a representative example of a species’ genetic material. Does not accurately represent the set of genes of any single organism, but a mosaic of different nucleic acid sequences from each individual. For each model organism, several possible reference genomes may be available (e.g. hg19 and hg38 for human). As the cost of DNA sequencing falls, and new full genome sequencing technologies emerge, more genome sequences continue to be generated. New alignments are built and the reference genomes improved (fewer gaps, fixed misrepresentations in the sequence, etc). The different reference genomes correspond to the different released versions (called “builds”).\n\nA mapper tool takes as input a reference genome and a set of reads. Its aim is to align each read on the reference genome, allowing mismatches, indels and clipping of some short fragments on the two ends of the reads.\n\nIllustration of the mapping process \n\nCurrently, there are over 60 different mappers, and their number is growing. Bowtie2 is fast and accurate aligner that we introduce for this purpose. It is an open-source tool particularly good at aligning sequencing reads of about 50 up to 1,000s of bases to relatively long genomes. By default, it performs a global end-to-end read alignment, and by changing the settings, it also supports the local alignment mode.\n\n\n\n\nBowtie2 can identify reads that are:  - uniquely mapped: read pairs aligned exactly 1 time  - multi-mapped: reads pairs aligned more than 1 time  - unmapped: read pairs non concordantly aligned or not aligned at all.\n\n\nMulti-mapped reads can happen because of repetition in the reference genome (e.g. multiple copies of a gene), particularly when reads are small. It is difficult to decide where these sequences come from and therefore most of the pipelines ignore them.\n\nChecking the mapping statistics is an important step to do before continuing any analyses. There are several potential sources for errors in mapping, including (but not limited to):\n\nPCR artifacts: PCR errors will show as mismatches in the alignment\nsequencing errors\nerror of the mapping algorithm due to repetitive regions or other low-complexity regions.\n\nA low percentage of uniquely mapped reads is often due to, (i) either excessive amplification in the PCR step, (ii) inadequate read length, or (iii) problems with the sequencing platform.\n\nFor percentage of uniquely mapped reads, 70% or higher is considered good, whereas 50% or lower is concerning. The percentages are not consistent across different organisms, thus the rule can be flexible!\n\nBut where the read mappings are stored?\n\n\nA BAM (Binary Alignment Map) file is a compressed binary file storing the read sequences, whether they have been aligned to a reference sequence (e.g. a chromosome), and if so, the position on the reference sequence at which they have been aligned.\nA BAM file (or a SAM file, the non-compressed version) consists of:\n\nA header section (the lines starting with @) containing metadata particularly the chromosome names and lengths (lines starting with the @SQ symbol)\nAn alignment section consisting of a table with 11 mandatory fields, as well as a variable number of optional fields.\n\n\nBAM file format \n\nQuestions:\n\n\nWhich information do you find in a BAM file that you also find in the FASTQ file?\n# Sequences and quality information\n\n\n\n\nWhat is the additional information compared to the FASTQ file?\n# Mapping information, Location of the read on the chromosome, Mapping quality, etc\n\n\n\n\n\n\nThe read count data generated by ChIP-seq is massive. So how to predict the DNA-binding sites from this read count data? For this, various Peak Calling methods have been developed. Peaks are regions with significant number of mapped reads that produce a pileup. Probably the most discussed issue in ChIP-seq experiments is the best method to find true peaks in the data. ChIP-seq is most often performed with single-end reads, and ChIP fragments are sequenced from their 5’ ends only. This creates two distinct peaks; one on each strand with the binding site falling in the middle of these peaks, the distance from the middle of the peaks to the binding site is often referred to as the “shift”.\n\n\nForward (blue) and reverse (maroon) Read Density Profiles derived from the read data contribute to the Combined Density Profile (orange). Nat. Methods,2008\n\n\nThe most popular method is MACS2 which empirically models the shift size of ChIP-Seq tags, and uses it to improve the spatial resolution of predicted binding sites. Briefly, these are the steps performed by MACS: - removing duplicate reads - modelling the shift size - scaling the libraries with respect to their controls - performing peak detection - estimating False Discovery Rate (FDR)\n\nThe peak calling step identifies areas in the genome that have been enriched with aligned reads as a result of performing ChIP-sequencing experiment.\n\nEnrichment = Immunoprecipitation reads/background reads (mock IP or untagged IP) If an experimental control data is NOT available, a random genomic background is assumed.\n\n\n\n\n\nOverview of the Peak Calling step\n\n\n\nFinally, peaks are filtered to reduce false positives and ranked according to relative strength or statistical significance.\nAfter peak calling, it’s important to check some metrics that are indicative of the quality of the ChIP-seq experiment. Here are two of the most useful:\n\nFRiP score: reports the percentage of reads overlapping within called peak. Can be useful to understand how much is “enriched” the IP sample.\nStrand cross-correlation: high-quality ChIP-seq produces significant clustering of enriched DNA sequence tags at locations bound by the protein of interest, that present as a bimodal enrichment of reads on the forward and reverse strands (peaks). The Cross-correlation Metric calculates how many bases to shift the peak in order to get the maximum correlation between the two peaks, which corresponds to the predominant fragment length.\n\n\n\n\nAn important factor that influences the read count that is required for a ChIP–seq experiment is whether the protein (or chromatin modification) is a point-source factor, a broad-source factor or a mixed-source factor.\n\nPoint sources occur at specific locations in the genome. This class includes sequence-specific transcription factors as well as some highly localized chromatin marks, for example, those associated with enhancers and transcription start sites. They will generate more often narrow peaks.\nBroad sources are generally those that cover extended areas of the genome, such as many chromatin marks (for example, histone H3 lysine 9 trimethylation (H3K9me3) marks). They will give raise to broad peaks.\nMixed-source factors, such as RNA polymerase II, yield both types of peaks. As expected, broad-source and mixed-source factors require a greater number of reads than point-source factors. Peaks generated will have a mixed profile between narrow and broad.\n\n\n\n\n\n\n\nMost high-throughput data can be viewed as a continuous score over the bases of the genome. In case of RNA-seq or ChIP-seq experiments, the data can be represented as read coverage values per genomic base position.\nThis sort of data can be stored as a generic text file or can have special formats such as Wig (stands for wiggle) from UCSC, or the bigWig format, which is an indexed binary format of the wig files. Another file that can be useful to inspect is the BigWig. This format is used to display large and continuous data\nThe bigWig format is great for data that covers a large fraction of the genome with varying scores, because the file is much smaller than regular text files that have the same information and it can be queried more easily since it is indexed.\n\nMost of the ENCODE project data can be downloaded in bigWig format!\n\nIf we want to know instead the peak locations in the genome as chromosome locations, we want to look at BED files, that are the main output of the peak calling step.\nA typical BED file is a text file format used to store genomic regions as coordinates. The data are presented in form of columns separated by tabs. This is the structure of a standard BED files, which can have also additional columns.\n\n\n\n\nWe will use this file type to inspect peak locations and for future downstream analyses!\n\n\n\n\nImagine that you are performing these steps of read processing not just for one library, but for a collection of samples with different replicates and experimental conditions, and you subsequently want to make differential comparisons across conditions. To this aim, it’s necessary to collapse the single peak files obtained from each single library into a single consensus peakset.\nIn this step you can also add a filtering to exclude genomic intervals that are identified as peaks only in a minority of samples. DiffBind and BEDTools are two common programs that can handle this task.\nIt can be helpful to make a plot of how many peaks overlap in how many samples, like this one:\n\n\nThis plot shows that there are almost 4000 total merged peaks, representing the union of all intervals. At the other extreme, there are around 100 peaks that overlap in all 11 samples, representing the intersection of all the samples.\n\n\nWhich should we chose? Given the rigor of the statistical analysis that we are going to perform, now we can choose a more inclusive consensus set. The default is to make the consensus peak set using peaks identified in at least two samples.\n\n\n\nNow that you have created a consensus peakset for all of your ChIP-seq experiment, it is time to actually count how many times a given sequence in your peakset is Your final dataset will represent a robust representation of binding events.\nOnce consensus peaks are defined, the next step is to quantify the number of sequencing reads that align to these regions. This read counting process provides quantitative information about the strength and extent of protein binding within the identified peaks.\n\nThe mapped reads can we counted across peaks in the consensus using a tool called FeatureCounts.\n\nCounts for the samples in all the consensus peakset are output as a tabular file, in which each row represents one peak, and you have one column for each sample. We will now load one and take a closer look, this will be our starting point in the hands-on analysis of a ChIP-seq dataset.\n\nBefore going on, let’s summarize the steps of a ChIP-seq analysis workflow that we have seen so far with the following scheme. You can find the file format names associated to the output of each task.\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn order to speed up the computations and keep memory usage low, we have subset the dataset only to chromosome 12. It will be interesting to see if we can recapitulate similar analyses by looking only to one chromosome 🤓\nData are in this public Google Drive folder. You will find:\n\nraw_counts_chr12.matrix: the peak by sample matrix containing the number of reads detected for each peak in each sample.\ncolData.txt: a tabular file containing our metadata related to columns of the count table, which contains different info about our samples (e.g. the treatment, the sample origin, etc.).\npeakset.bed: a BED file with the chromosome locations of all the consensus peakset.\nrecurrence_chr12.matrix: this is a table where you can find if any of the regions that constitute the consensus peakset is also called as peak in each separate sample. Remember that when a consensus peakset is created, usually genomic intervals called ad peaks in at least 2 out of all the samples are kept in the consensus, and you might find also intervals that are called in 3 or in all the samples. It might be interesting to check how the data are distributed.\ndba_Korg_Ntissue_homer_annot.txt: this file contains information about the annotation of each consensus peakset to the nearest gene TSS. You will understand later its usage.\nKorg_UP_regions_results.txt and Ncr_UP_regions_results.txt: these files store the differential analysis results for the entire peakset and we will need them on day 3 to perform downstream functional analyses.\n\nOpen the folder through Google Drive, check the presence of the files in the browser and THEN run the code below.\n\nAfter you run the code below, look into your R console and check if you are prompted to insert your Google account information. Do so and then follow the instructions to connect to your Google account in order to download the data from the shared folder!\n\n\n# Load installed packages with the \"library()\" function\nlibrary(dplyr)\nlibrary(googledrive)\n\n# Load files\nfiles &lt;- drive_ls(path=\"MOBW2024_uploadata\")\n\n# File paths with URL\ncounts &lt;- files[files$name == \"raw_counts_chr12.matrix\",] %&gt;% drive_read_string() %&gt;% read.table(text = ., sep=\"\\t\") %&gt;% as.data.frame()\n\nsamples &lt;- files[files$name == \"colData.txt\",] %&gt;% drive_read_string() %&gt;% read.table(text = ., sep=\"\\t\") %&gt;% as.data.frame()\n\nWe can now explore the data that we have just loaded in the current R session to familiarize with it.\n\n# Check out the counts\nhead(counts, 10)\n\n\n\n\n\n\n\n\nSQ_2157\nSQ_1990\nSQ_2010\nSQ_2163\nSQ_2204\nSQ_2212\nSQ_2216\nSQ_2222\nSQ_2288\nSQ_2303\nSQ_2298\nSQ_2145\nGSM2058021\nGSM2058022\nGSM2058023\n\n\n\n\nreg_6364\n8\n1\n1\n97\n1\n13\n3\n12\n24\n19\n47\n4\n85\n34\n185\n\n\nreg_6365\n26\n20\n27\n47\n58\n67\n47\n52\n65\n21\n43\n11\n79\n56\n216\n\n\nreg_6366\n1\n9\n1\n6\n1\n1\n1\n7\n1\n1\n45\n1\n65\n44\n169\n\n\nreg_6367\n1\n1\n1\n21\n2\n14\n4\n5\n2\n1\n52\n29\n132\n66\n188\n\n\nreg_6368\n1\n1\n15\n17\n6\n10\n6\n1\n11\n5\n11\n3\n11\n23\n56\n\n\nreg_6369\n184\n304\n215\n661\n278\n519\n466\n435\n500\n267\n1045\n297\n1950\n1562\n3860\n\n\nreg_6370\n28\n1\n2\n92\n15\n21\n19\n50\n49\n1\n57\n9\n91\n44\n136\n\n\nreg_6371\n141\n290\n281\n673\n196\n398\n438\n303\n451\n188\n666\n222\n1793\n1409\n3416\n\n\nreg_6372\n1\n1\n3\n26\n3\n1\n1\n1\n1\n1\n39\n1\n39\n25\n222\n\n\nreg_6373\n63\n86\n112\n293\n70\n168\n55\n99\n268\n135\n187\n31\n580\n229\n894\n\n\n\n\n\n\n\n\nWe can then check the shape of our counts table (i.e. how many different peaks we are detecting and how many different samples?)\n\n# How many rows and columns does our count table have?\ndim(counts)\n\n[1] 1581   15\n\n\nWe can see that our table contains count information for 1581 peaks and 15 samples.\nWe can also inspect the metadata from the samples which is stored in the samples variable we created above.\n\n# What does the table look like?\nsamples\n\n\n\n\n\n\n\n\ngroups\nsizeFactor\nTissue\nSampleID\nTreatment\n\n\n\n\nSQ_2157\nK_org\n2.489936\n4KE\nSQ_2157\nK_org\n\n\nSQ_1990\nK_org\n2.758060\n8KE\nSQ_1990\nK_org\n\n\nSQ_2010\nK_org\n2.760352\n10KE\nSQ_2010\nK_org\n\n\nSQ_2163\nK_org\n2.425892\n13KE\nSQ_2163\nK_org\n\n\nSQ_2204\nK_org\n1.863534\n18KE\nSQ_2204\nK_org\n\n\nSQ_2212\nK_org\n2.580778\n11KW\nSQ_2212\nK_org\n\n\nSQ_2216\nK_org\n2.159358\n24KE\nSQ_2216\nK_org\n\n\nSQ_2222\nK_org\n2.203927\n22KE\nSQ_2222\nK_org\n\n\nSQ_2288\nK_org\n2.402964\n36KE\nSQ_2288\nK_org\n\n\nSQ_2303\nK_org\n2.430445\n41KE\nSQ_2303\nK_org\n\n\nSQ_2298\nN_crypts\n3.756529\nCR_41_mp\nSQ_2298\nN_crypts\n\n\nSQ_2145\nN_crypts\n2.804850\nCR_28_mp\nSQ_2145\nN_crypts\n\n\nGSM2058021\nN_crypts\n1.252530\nCR_28\nGSM2058021\nN_crypts\n\n\nGSM2058022\nN_crypts\n1.000000\nCR_29\nGSM2058022\nN_crypts\n\n\nGSM2058023\nN_crypts\n2.476903\nCR_37\nGSM2058023\nN_crypts\n\n\n\n\n\n\n\n\n\n# What is the shape of this samples table?\ndim(samples)\n\n[1] 15  5\n\n\nIn this case, this samples table has as many rows (15) as there are samples (which in turn is equal to the number of columns in the counts table), with columns containing different types of information related to each of the samples in the analysis.\n\n\n\nLet’s save this object with samples information in a file on this cloud session, this might be needed later if we end up in some trouble with the R session! This is a file format where columns are separated by commas. You might be familiar with this format if you have worked quite a bit in Excel. In R, we can save tabular data with the write.table() function specifying the location (the file name) we want. This is useful in the case our R session dies or we decide to interrupt it. In this case we will not have to run the whole analysis from the beginning and we can just source the file and load it!\n\nwrite.table(samples, \"samples_table.csv\", sep = \",\", quote = FALSE)\n\nWe can load the object back into the current session by using the following code line:\n\nsamples &lt;- read.table(\"samples_table.csv\", sep = \",\")\n\n\nWe will also repeat this procedure with the results of the differential expression analysis in order to avoid repeating work we have already done in case of any trouble!"
  },
  {
    "objectID": "pages/Day1.html#the-chip-seq-technology",
    "href": "pages/Day1.html#the-chip-seq-technology",
    "title": "Day 1",
    "section": "",
    "text": "Chromatin immunoprecipitation (ChIP) assays are used to analyze protein interactions with DNA. By combining this method with Next Generation Sequencing, ChIP-sequencing (ChIP-seq) is a powerful method for identifying genome-wide the chromosomal locations of different kinds of proteins, including transcription factors, nucleosomes, chromatin remodeling enzymes and polymerases, and also histone modifications.\n\nChIP-seq use in mapping histone modifications has been seminal in epigenetics research! (Ku et al., 2011)\n\nThe standard ChIP-seq protocol first cross-link bound proteins to chromatin, fragments the chromatin, captures the DNA fragments bound to one protein using an antibody specific to it, and sequences the ends of the captured fragments using NGS. Here you can see the major steps:\n+ Cross-linking: add Formaldehyde\n+ Hydrolysis (breakdown of cell memberane)\n+ Lyses of DNA fragments (shearing), e.g. treat with specific endonuclease enzymes (dpn1)\n+ Add specific antibody (with beads) against the protein of interest. This will drag the protein down by immuno-precipitation\n+ Separate DNA from the protein (unlinking), and purify the DNA\n+ DNA fragments are amplified and fluorescently tagged, library construction, and fragments are then sequenced"
  },
  {
    "objectID": "pages/Day1.html#tips-for-designing-a-chip-seq-experiment",
    "href": "pages/Day1.html#tips-for-designing-a-chip-seq-experiment",
    "title": "Day 1",
    "section": "",
    "text": "A common goal in ChIP-seq experiments is to identify changes in protein binding patterns between conditions (i.e. differential binding).\nEffective analysis of ChIP-seq data requires sufficient coverage by sequence reads (sequencing depth). It mainly depends on the size of the genome, and the number and size of the binding sites of the protein.\n\nFor mammalian transcription factors (TFs) and chromatin modifications such as enhancer-associated histone marks: 20 million reads are adequate\nProteins with more binding sites (e.g., RNA Pol II) or broader factors: need more reads, up to 60 million for mammalian ChIP-seq\nSequencing depth rules of thumb: &gt;10M reads for narrow peaks, &gt;20M for broad peaks\nLong & paired-end reads useful but not essential\nReplicates: The good news here is that, unlike RNA-Seq, more than 2 replicates does not significantly increase the number of targets\n\nA successful ChIP-seq experiment must have its right controls: they are essential to eliminate false positives as there is always some signal on open chromatin. Two types of controls are often used:\n\nAn “input” DNA sample (fragmented genomic DNA), the one that has been cross-linked and sonicated but not immuno-precipitated\nAn IgG “mock”-ChIP, using an antibody that will not bind to nuclear proteins and should generate random immuno-precipitated DNA.\n\n\nInput DNA control is ideal in most of the cases. Indeed, one problem with IgG control is that if too little DNA is recovered after immuno-precipitation, sequencing library will be of low complexity and binding sites identified using this control could be biased. Read this biostars post for discussion."
  },
  {
    "objectID": "pages/Day1.html#next-generation-sequencing",
    "href": "pages/Day1.html#next-generation-sequencing",
    "title": "Day 1",
    "section": "",
    "text": "NGS technologies (Illumina/PacBio) allow the processing of millions of reactions in parallel, resulting in high-throughput, higher sensitivity, speed and reduced cost compared to first generation sequencing technologies (e.g., Sanger method).\n\nGiven the vast amounts of quantitative sequencing data generated, NGS-based methods rely on resource-intensive data processing pipelines to analyze data.\n\nIn addition to the sequence itself, and unlike Sanger sequencing, the high-throughput nature of NGS provides quantitative information (depth of coverage) due to the high level of sequence redundancy at a locus.\nThere are short-read and long-read NSG approaches. Short reads of NGS range in size from 75 to 300 bp depending on the application and sequencing chemistry. NGS is taken to mean second generation technologies, however 3G and 4G technologies have since evolved (enable longer read sequences in excess of 10 kilobases).\nAmong 2G NGS chemistries, Illumina sequencing by synthesis (SBS) is the most widely adopted worldwide, responsible for generating more than 90% of the world’s sequencing data."
  },
  {
    "objectID": "pages/Day1.html#setting-up-rstudio",
    "href": "pages/Day1.html#setting-up-rstudio",
    "title": "Day 1",
    "section": "",
    "text": "First of all, let’s start assembling the tools that you will need for this workshop! While R by itself has no particular user interface, RStudio is the tool that allows you to click and actually ‘see’ what you are doing. RStudio is an integrated development environment (IDE) and will be our primary tool to interact with R.\nYou can follow two strategies:\n\nInstall R and RStudio on your machine\nRun RStudio in a browser using RStudio Cloud.\n\nNote that the first option will require a bit more time and effort. To get started with RStudio Cloud (noew Posit Cloud), click on this link and create a Posit free account. Once you completed your registration, you are redirected to Your Workspace. To start a new R project, you can click on New Project &gt; New RStudio Project. This will open RStudio on the Cloud."
  },
  {
    "objectID": "pages/Day1.html#the-rstudio-interface",
    "href": "pages/Day1.html#the-rstudio-interface",
    "title": "Day 1",
    "section": "",
    "text": "Now that you have RStudio open, you should see something similar to this:\n\n\n\nThe RStudio interface is composed of quadrants:\n\nThe Console window: located in the bottom-left, it’s where you will find the output of your coding, and it’s also possible to type R code interactively on the console.\nThe Files / Plots / Packages / Help / Viewer window: located in the bototm-right, it currently displays your current file system (on the Cloud), which is pointed to the position where your Rproject you just created lives.\n\nThe Files pane lists the files and folders in your root directory (i.e. where by default RStudio save your analyses).\nAnother important pane is Plots, designed to show you any plots you have created.\nThe Package pane instead allows to manage all the additional tools you can import and use when you perform your analyses. They are useful to enhance the basic R functions from different aspects.\nIf you need H E L P, you can use the Help pane: you can search for specific topics, for instance how a computation works and other kinds of documentation.\n\nThe Environment / History / Connections / Tutorial window: you can find it in the top right.\n\nIt is pointed to the Environment pane, that is a place where you can find all the objects available for computation (e.g. datasets, plots, lists, etc.).\nThe History pane keeps whatever computation you run in the console. You can rerun the computation stored in the history and you can also copy the past code into an existing or new R Script."
  },
  {
    "objectID": "pages/Day1.html#r-fundamentals",
    "href": "pages/Day1.html#r-fundamentals",
    "title": "Day 1",
    "section": "",
    "text": "The most basic computation you can do in R is arithmetic operations. Let’s take a look at the following examples:\n\n# Addition\n14 + 7\n\n[1] 21\n\n# Division\n14 / 7\n\n[1] 2\n\n# Square root\nsqrt(14)\n\n[1] 3.741657\n\n\nBut R is much more than an enormous calculator! Besides arithmetic operations, there are also logical queries you can perform. Logical queries always return either the value TRUE or FALSE.\n\n#1 Is it TRUE or FALSE?\n3 == 3\n\n[1] TRUE\n\n#2 Is 45 bigger than 55?\n50 &gt; 56\n\n[1] FALSE\n\n#3 Are these two words NOT the same?\n\"Gene\" != \"gene\"\n\n[1] TRUE\n\n\n\nNotice that in logical queries, equal to is represented by == and not by = , which instead is used to assign a value to an object. However, in practice, most R programmers tend to avoid = since it can easily lead to confusion with ==.\n\n\n\n\nAssigning values to objects is another common task that we will perform. What can be an object ?\n\na list of names\na series of number\na dataset\na plot\na function\netc.\n\nIn short, an object can enclose different things which form part of your data analysis. For example, objects are useful to store results that you want to process further in later steps of the analysis. We have to use the assign operator &lt;- to assign a value to an object. Let’s have a look at an example.\n\n# Let's assign the gene \"TP53\" to the \"genes\" object\ngenes &lt;- \"TP53\"\n\nNow the object genes carries the value \"TP53\". If you check out the Environment pane, you should see that the gene object appeared there. We can also print values of an object in the console by simply typing the name of the object genes and hit Return ↵.\n\n# What are my genes?\ngenes\n\n[1] \"TP53\"\n\n\nTo create objects with multiple values, we can use the function c() which stands for ‘concatenate’:\n\n# Adding some more genes to the object\ngenes &lt;- c(\"TP53\",\n           \"TNF\",\n           \"EGFR\",\n           \"VEGFA\",\n           \"APOE\",\n           \"IL6\")\n           \n# Here are my genes\ngenes\n\n[1] \"TP53\"  \"TNF\"   \"EGFR\"  \"VEGFA\" \"APOE\"  \"IL6\"  \n\n\nTo concatenate values into a single object, we need to use a comma , to separate each value. Otherwise, R will report an error back.\n\n\ngenes &lt;- c(\"TP53\" \"TNF\")\n\nError: &lt;text&gt;:1:19: unexpected string constant\n1: genes &lt;- c(\"TP53\" \"TNF\"\n                      ^\n\n\n\nDon’t worry about it! R’s error messages tend to be very useful and give meaningful clues to what went wrong. In this case, we can see that something ‘unexpected’ happened, and it shows where our mistake is."
  },
  {
    "objectID": "pages/Day1.html#object-types-list",
    "href": "pages/Day1.html#object-types-list",
    "title": "Day 1",
    "section": "",
    "text": "Values inside objects are meant to be of the same type, for instance numeric or character. Consequently, mixing different types of data into one object is likely a bad idea. However, there is an exception: a list object can hold other objects of different data types.\n\ndata &lt;- list(genes,\n             c(1, 2, 3, 4),\n             c(\"TGFB1\", 5, \"AKT1\", 6))\ndata\n\n[[1]]\n[1] \"TP53\"  \"TNF\"   \"EGFR\"  \"VEGFA\" \"APOE\"  \"IL6\"  \n\n[[2]]\n[1] 1 2 3 4\n\n[[3]]\n[1] \"TGFB1\" \"5\"     \"AKT1\"  \"6\"    \n\n\nYou will notice in this example that I do not use \"\" for each value in the list. This is because genes is not a character value, but an object. When we refer to objects, we do not need quotation marks.\nLooking at the list item in the environment page, you can see that the object data is classified as a List of 3, and if you click on the blue icon, you can inspect the different objects inside.\n\n\n\n\n\nWe mention the term ‘function’ multiple times, but what are functions and why we need them? In simple terms, functions are objects that contain lines of codes and can be seen as shortcuts for our programming. They are useful to save space and time and to make our code more readable and reliable. We will make an example to understand better, by calculating the arithmetic mean:\n\n# We create an object that stores our values\nvalues &lt;- c(1, 5, 7, 4, 13, 2, 9, 5)\n\n# Manually compute the sum \nsum &lt;- 1 + 5 + 7 + 4 + 13 + 2 + 9 + 5\n\n# Divide the sum by the number of values\nmean &lt;- sum / 8\nmean\n\n[1] 5.75\n\n\nOr…\n\n# Use the mean function that is part of the R base package\nmean(values)\n\n[1] 5.75\n\n\n\n# Let's check that the two methods coincide\nsum / 8 == mean(values)\n\n[1] TRUE\n\n\n\n\n\nThe last type of objects we will see (of course there are far more types…) is the data.frame, one of the most abundantly used in R. Think of data.frames as the R equivalent of Excel spreadsheets, so a way to store tabular data. As we will see later, pretty much all the data we are going to handle will be in the form of a data.frame or some of its other variations.\n\n# Let's create and display a data frame (a table) with four rows and two columns\ndata.frame(\"Class\"=c(\"a\",\"b\",\"c\",\"d\"), # First column\n            \"Quantity\"=c(1,10,4,6)) # Second column\n\n  Class Quantity\n1     a        1\n2     b       10\n3     c        4\n4     d        6"
  },
  {
    "objectID": "pages/Day1.html#create-an-rscript",
    "href": "pages/Day1.html#create-an-rscript",
    "title": "Day 1",
    "section": "",
    "text": "You might want to save your code for a later use, especially when it starts to become long and complex. In this case, it is not very convenient to write it in the console, which does not keep track of it (as far as you don’t go back in the History) and does not allow to edit the code and save changes.\nSo, instead, we can write code into an R Script, which is a document that RStudio recognises as R programming code and has .R as extension. Files that are not R Scripts, like .txt, .rtf or .md, can also be opened in RStudio, but any code written in it will not be automatically recognized.\nWhen opening an R script or creating a new one, it will display in the Source window. The term ‘source’ can be understood as any type of file, e.g. data, programming code, notes, etc.\n\nNow let’s create an R script, by selecting File &gt; New File &gt; R Script in the menu bar, or using the keyboard shortcut Ctrl + Shift + N on PC and Cmd + Shift + N on Mac. We will name it Day1.R.\nNow you can go in the History, copy the lines previously coded (by clicking on the first line you want to copy, then press Shift + Down arrow, up to the last line of code and then clicking To Source.\n❗️ Writing some code in your R script will NOT automatically run it! If you tried pressing Return ↵, you would only add a new line. Instead, you need to select the code you want to run and press Ctrl+Return ↵ (PC) or Cmd+Return ↵ (Mac)."
  },
  {
    "objectID": "pages/Day1.html#install-the-packages",
    "href": "pages/Day1.html#install-the-packages",
    "title": "Day 1",
    "section": "",
    "text": "The analyses we are going to perform require specific functions that are not included the basic set of functions in R. These functions are collected in specific packages. R packages are extensions to the R programming language that contain code, data and documentation which help us perform standardized workflows. In the chunk below, we instruct R to install the packages that we will need later on through the workshop.\n\nCopy this chunk and paste it to your R script.\n\n\n# Install packages from Bioconductor\nif (!require(\"BiocManager\", quietly = TRUE))\n  install.packages(\"BiocManager\")\n\n# Install packages from CRAN\ninstall.packages(\"tidyr\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"googledrive\")\n\n# For differential analysis\nBiocManager::install(\"vsn\")\nBiocManager::install(\"edgeR\")\ninstall.packages(\"statmod\")\n\n# For visualizations\ninstall.packages(\"hexbin\")\ninstall.packages(\"pheatmap\")\ninstall.packages(\"RColorBrewer\")\ninstall.packages(\"ggrepel\")\ninstall.packages(\"circlize\")\n\n# For downstream analysis\ninstall.packages(\"gprofiler2\")\n\n# Clean garbage\ngc()\n\nDuring the installation, you will see many messages being displayed on your R console, don’t pay too much attention to them unless they are red and specify an error!\nIf you encounter any of these messages during installation, follow this procedure here:\n\n# R asks for package updates, answer \"n\" and type enter\n# Question displayed:\nUpdate all/some/none? [a/s/n]:\n\n# Answer to type:  \nn\n\n# R asks for installation from binary source, answer \"no\" and type enter\n# Question displayed:\nDo you want to install from sources the packages which need compilation? (Yes/no/cancel)\n\n# Answer to type:\nno\n\nWhile the packages are installed, we can start diving into the ChIP-seq core processing steps!"
  },
  {
    "objectID": "pages/Day1.html#the-basics-of-chip-seq-data-core-processing",
    "href": "pages/Day1.html#the-basics-of-chip-seq-data-core-processing",
    "title": "Day 1",
    "section": "",
    "text": "The raw output of any sequencing run consists of a series of sequences (called reads). These sequences can have varying length based on the run parameters set on the sequencing platform. Nevertheless, they are made available for humans to read under a standardized file format known as FASTQ. This is the universally accepted format used to encode sequences after sequencing. An example of real FASTQ file with only two reads is provided below.\n\n@Seq1\nAGTCAGTTAAGCTGGTCCGTAGCTCTGAGGCTGACGAGTCGAGCTCGTACG\n+\nBBBEGGGGEGGGFGFGGEFGFGFGGFGGGGGGFGFGFGGGFGFGFGFGFG\n@Seq2\nTGCTAAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGC\n+\nEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n\nFASTQ files are an intermediate file in the analysis and are used to assess quality metrics for any given sequence. The quality of each base call is encoded in the line after the + following the standard Phred score system.\n\n💡 Since we now have an initial metric for each sequence, it is mandatory to conduct some standard quality control evaluation of our sequences to eventually spot technical defects in the sequencing run early on in the analysis."
  },
  {
    "objectID": "pages/Day1.html#quality-metrics-inspection",
    "href": "pages/Day1.html#quality-metrics-inspection",
    "title": "Day 1",
    "section": "",
    "text": "Computational tools like FastQC aid with the visual inspection of per-sample quality metrics from NGS experiments. Some of the QC metrics of interest to consider include the ones listed below, on the left are optimal metric profiles while on the right are sub-optimal ones:\n\nPer-base Sequence Quality  This uses box plots to highlight the per-base quality along all reads in the sequencing experiment, we can notice a physiological drop in quality towards the end part of the read.\n\n\nPer-sequence Quality Scores  Here we are plotting the distribution of Phred scores across all identified sequences, we can see that the high quality experiment (left) has a peak at higher Phred scores values (34-38).\n\n\nPer-base Sequence Content  Here we check the sequence (read) base content, in a normal scenario we do not expect any dramatic variation across the full length of the read since we should see a quasi-balanced distribution of bases.\n\n\nPer-sequence GC Content  GC-content referes to the degree at which guanosine and cytosine are present within a sequence, in NGS experiments which also include PCR amplification this aspect is crucial to check since GC-poor sequences may be enriched due to their easier amplification bias. In a normal random library we would expect this to have a bell-shaped distribution such as the one on the left.\n\n\nSequence Duplication Levels  This plot shows the degree of sequence duplication levels. In a normal library (left) we expect to have low levels of duplication which can be a positive indicator of high sequencing coverage.\n\n\nAdapter Content  In NGS experiments we use adapters to create a library. Sometimes these can get sequenced accidentally and end up being part of a read. This phenomenon can be spotted here and corrected using a computational approach called adapter trimming.\n\n\nLibrary complexity is also a common quality measure for ChIP-seq libraries. It is linked to many factors such as antibody quality, over-crosslinking, amount of material, sonication, or over-amplification by PCR."
  },
  {
    "objectID": "pages/Day1.html#read-alignment",
    "href": "pages/Day1.html#read-alignment",
    "title": "Day 1",
    "section": "",
    "text": "Now that we have assessed the quality of the sequencing data, we are ready to align the reads to the reference genome.\n\nWhat is a reference genome? A set of nucleic acid sequences assembled as a representative example of a species’ genetic material. Does not accurately represent the set of genes of any single organism, but a mosaic of different nucleic acid sequences from each individual. For each model organism, several possible reference genomes may be available (e.g. hg19 and hg38 for human). As the cost of DNA sequencing falls, and new full genome sequencing technologies emerge, more genome sequences continue to be generated. New alignments are built and the reference genomes improved (fewer gaps, fixed misrepresentations in the sequence, etc). The different reference genomes correspond to the different released versions (called “builds”).\n\nA mapper tool takes as input a reference genome and a set of reads. Its aim is to align each read on the reference genome, allowing mismatches, indels and clipping of some short fragments on the two ends of the reads.\n\nIllustration of the mapping process \n\nCurrently, there are over 60 different mappers, and their number is growing. Bowtie2 is fast and accurate aligner that we introduce for this purpose. It is an open-source tool particularly good at aligning sequencing reads of about 50 up to 1,000s of bases to relatively long genomes. By default, it performs a global end-to-end read alignment, and by changing the settings, it also supports the local alignment mode.\n\n\n\n\nBowtie2 can identify reads that are:  - uniquely mapped: read pairs aligned exactly 1 time  - multi-mapped: reads pairs aligned more than 1 time  - unmapped: read pairs non concordantly aligned or not aligned at all.\n\n\nMulti-mapped reads can happen because of repetition in the reference genome (e.g. multiple copies of a gene), particularly when reads are small. It is difficult to decide where these sequences come from and therefore most of the pipelines ignore them.\n\nChecking the mapping statistics is an important step to do before continuing any analyses. There are several potential sources for errors in mapping, including (but not limited to):\n\nPCR artifacts: PCR errors will show as mismatches in the alignment\nsequencing errors\nerror of the mapping algorithm due to repetitive regions or other low-complexity regions.\n\nA low percentage of uniquely mapped reads is often due to, (i) either excessive amplification in the PCR step, (ii) inadequate read length, or (iii) problems with the sequencing platform.\n\nFor percentage of uniquely mapped reads, 70% or higher is considered good, whereas 50% or lower is concerning. The percentages are not consistent across different organisms, thus the rule can be flexible!\n\nBut where the read mappings are stored?\n\n\nA BAM (Binary Alignment Map) file is a compressed binary file storing the read sequences, whether they have been aligned to a reference sequence (e.g. a chromosome), and if so, the position on the reference sequence at which they have been aligned.\nA BAM file (or a SAM file, the non-compressed version) consists of:\n\nA header section (the lines starting with @) containing metadata particularly the chromosome names and lengths (lines starting with the @SQ symbol)\nAn alignment section consisting of a table with 11 mandatory fields, as well as a variable number of optional fields.\n\n\nBAM file format \n\nQuestions:\n\n\nWhich information do you find in a BAM file that you also find in the FASTQ file?\n# Sequences and quality information\n\n\n\n\nWhat is the additional information compared to the FASTQ file?\n# Mapping information, Location of the read on the chromosome, Mapping quality, etc"
  },
  {
    "objectID": "pages/Day1.html#peak-calling",
    "href": "pages/Day1.html#peak-calling",
    "title": "Day 1",
    "section": "",
    "text": "The read count data generated by ChIP-seq is massive. So how to predict the DNA-binding sites from this read count data? For this, various Peak Calling methods have been developed. Peaks are regions with significant number of mapped reads that produce a pileup. Probably the most discussed issue in ChIP-seq experiments is the best method to find true peaks in the data. ChIP-seq is most often performed with single-end reads, and ChIP fragments are sequenced from their 5’ ends only. This creates two distinct peaks; one on each strand with the binding site falling in the middle of these peaks, the distance from the middle of the peaks to the binding site is often referred to as the “shift”.\n\n\nForward (blue) and reverse (maroon) Read Density Profiles derived from the read data contribute to the Combined Density Profile (orange). Nat. Methods,2008\n\n\nThe most popular method is MACS2 which empirically models the shift size of ChIP-Seq tags, and uses it to improve the spatial resolution of predicted binding sites. Briefly, these are the steps performed by MACS: - removing duplicate reads - modelling the shift size - scaling the libraries with respect to their controls - performing peak detection - estimating False Discovery Rate (FDR)\n\nThe peak calling step identifies areas in the genome that have been enriched with aligned reads as a result of performing ChIP-sequencing experiment.\n\nEnrichment = Immunoprecipitation reads/background reads (mock IP or untagged IP) If an experimental control data is NOT available, a random genomic background is assumed.\n\n\n\n\n\nOverview of the Peak Calling step\n\n\n\nFinally, peaks are filtered to reduce false positives and ranked according to relative strength or statistical significance.\nAfter peak calling, it’s important to check some metrics that are indicative of the quality of the ChIP-seq experiment. Here are two of the most useful:\n\nFRiP score: reports the percentage of reads overlapping within called peak. Can be useful to understand how much is “enriched” the IP sample.\nStrand cross-correlation: high-quality ChIP-seq produces significant clustering of enriched DNA sequence tags at locations bound by the protein of interest, that present as a bimodal enrichment of reads on the forward and reverse strands (peaks). The Cross-correlation Metric calculates how many bases to shift the peak in order to get the maximum correlation between the two peaks, which corresponds to the predominant fragment length."
  },
  {
    "objectID": "pages/Day1.html#broad-vs.-narrow-peaks",
    "href": "pages/Day1.html#broad-vs.-narrow-peaks",
    "title": "Day 1",
    "section": "",
    "text": "An important factor that influences the read count that is required for a ChIP–seq experiment is whether the protein (or chromatin modification) is a point-source factor, a broad-source factor or a mixed-source factor.\n\nPoint sources occur at specific locations in the genome. This class includes sequence-specific transcription factors as well as some highly localized chromatin marks, for example, those associated with enhancers and transcription start sites. They will generate more often narrow peaks.\nBroad sources are generally those that cover extended areas of the genome, such as many chromatin marks (for example, histone H3 lysine 9 trimethylation (H3K9me3) marks). They will give raise to broad peaks.\nMixed-source factors, such as RNA polymerase II, yield both types of peaks. As expected, broad-source and mixed-source factors require a greater number of reads than point-source factors. Peaks generated will have a mixed profile between narrow and broad."
  },
  {
    "objectID": "pages/Day1.html#bed-and-bigwig-file-formats",
    "href": "pages/Day1.html#bed-and-bigwig-file-formats",
    "title": "Day 1",
    "section": "",
    "text": "Most high-throughput data can be viewed as a continuous score over the bases of the genome. In case of RNA-seq or ChIP-seq experiments, the data can be represented as read coverage values per genomic base position.\nThis sort of data can be stored as a generic text file or can have special formats such as Wig (stands for wiggle) from UCSC, or the bigWig format, which is an indexed binary format of the wig files. Another file that can be useful to inspect is the BigWig. This format is used to display large and continuous data\nThe bigWig format is great for data that covers a large fraction of the genome with varying scores, because the file is much smaller than regular text files that have the same information and it can be queried more easily since it is indexed.\n\nMost of the ENCODE project data can be downloaded in bigWig format!\n\nIf we want to know instead the peak locations in the genome as chromosome locations, we want to look at BED files, that are the main output of the peak calling step.\nA typical BED file is a text file format used to store genomic regions as coordinates. The data are presented in form of columns separated by tabs. This is the structure of a standard BED files, which can have also additional columns.\n\n\n\n\nWe will use this file type to inspect peak locations and for future downstream analyses!"
  },
  {
    "objectID": "pages/Day1.html#collapse-data-into-a-single-dataset",
    "href": "pages/Day1.html#collapse-data-into-a-single-dataset",
    "title": "Day 1",
    "section": "",
    "text": "Imagine that you are performing these steps of read processing not just for one library, but for a collection of samples with different replicates and experimental conditions, and you subsequently want to make differential comparisons across conditions. To this aim, it’s necessary to collapse the single peak files obtained from each single library into a single consensus peakset.\nIn this step you can also add a filtering to exclude genomic intervals that are identified as peaks only in a minority of samples. DiffBind and BEDTools are two common programs that can handle this task.\nIt can be helpful to make a plot of how many peaks overlap in how many samples, like this one:\n\n\nThis plot shows that there are almost 4000 total merged peaks, representing the union of all intervals. At the other extreme, there are around 100 peaks that overlap in all 11 samples, representing the intersection of all the samples.\n\n\nWhich should we chose? Given the rigor of the statistical analysis that we are going to perform, now we can choose a more inclusive consensus set. The default is to make the consensus peak set using peaks identified in at least two samples.\n\n\n\nNow that you have created a consensus peakset for all of your ChIP-seq experiment, it is time to actually count how many times a given sequence in your peakset is Your final dataset will represent a robust representation of binding events.\nOnce consensus peaks are defined, the next step is to quantify the number of sequencing reads that align to these regions. This read counting process provides quantitative information about the strength and extent of protein binding within the identified peaks.\n\nThe mapped reads can we counted across peaks in the consensus using a tool called FeatureCounts.\n\nCounts for the samples in all the consensus peakset are output as a tabular file, in which each row represents one peak, and you have one column for each sample. We will now load one and take a closer look, this will be our starting point in the hands-on analysis of a ChIP-seq dataset.\n\nBefore going on, let’s summarize the steps of a ChIP-seq analysis workflow that we have seen so far with the following scheme. You can find the file format names associated to the output of each task."
  },
  {
    "objectID": "pages/Day1.html#load-and-explore-the-dataset",
    "href": "pages/Day1.html#load-and-explore-the-dataset",
    "title": "Day 1",
    "section": "",
    "text": "In order to speed up the computations and keep memory usage low, we have subset the dataset only to chromosome 12. It will be interesting to see if we can recapitulate similar analyses by looking only to one chromosome 🤓\nData are in this public Google Drive folder. You will find:\n\nraw_counts_chr12.matrix: the peak by sample matrix containing the number of reads detected for each peak in each sample.\ncolData.txt: a tabular file containing our metadata related to columns of the count table, which contains different info about our samples (e.g. the treatment, the sample origin, etc.).\npeakset.bed: a BED file with the chromosome locations of all the consensus peakset.\nrecurrence_chr12.matrix: this is a table where you can find if any of the regions that constitute the consensus peakset is also called as peak in each separate sample. Remember that when a consensus peakset is created, usually genomic intervals called ad peaks in at least 2 out of all the samples are kept in the consensus, and you might find also intervals that are called in 3 or in all the samples. It might be interesting to check how the data are distributed.\ndba_Korg_Ntissue_homer_annot.txt: this file contains information about the annotation of each consensus peakset to the nearest gene TSS. You will understand later its usage.\nKorg_UP_regions_results.txt and Ncr_UP_regions_results.txt: these files store the differential analysis results for the entire peakset and we will need them on day 3 to perform downstream functional analyses.\n\nOpen the folder through Google Drive, check the presence of the files in the browser and THEN run the code below.\n\nAfter you run the code below, look into your R console and check if you are prompted to insert your Google account information. Do so and then follow the instructions to connect to your Google account in order to download the data from the shared folder!\n\n\n# Load installed packages with the \"library()\" function\nlibrary(dplyr)\nlibrary(googledrive)\n\n# Load files\nfiles &lt;- drive_ls(path=\"MOBW2024_uploadata\")\n\n# File paths with URL\ncounts &lt;- files[files$name == \"raw_counts_chr12.matrix\",] %&gt;% drive_read_string() %&gt;% read.table(text = ., sep=\"\\t\") %&gt;% as.data.frame()\n\nsamples &lt;- files[files$name == \"colData.txt\",] %&gt;% drive_read_string() %&gt;% read.table(text = ., sep=\"\\t\") %&gt;% as.data.frame()\n\nWe can now explore the data that we have just loaded in the current R session to familiarize with it.\n\n# Check out the counts\nhead(counts, 10)\n\n\n\n\n\n\n\n\nSQ_2157\nSQ_1990\nSQ_2010\nSQ_2163\nSQ_2204\nSQ_2212\nSQ_2216\nSQ_2222\nSQ_2288\nSQ_2303\nSQ_2298\nSQ_2145\nGSM2058021\nGSM2058022\nGSM2058023\n\n\n\n\nreg_6364\n8\n1\n1\n97\n1\n13\n3\n12\n24\n19\n47\n4\n85\n34\n185\n\n\nreg_6365\n26\n20\n27\n47\n58\n67\n47\n52\n65\n21\n43\n11\n79\n56\n216\n\n\nreg_6366\n1\n9\n1\n6\n1\n1\n1\n7\n1\n1\n45\n1\n65\n44\n169\n\n\nreg_6367\n1\n1\n1\n21\n2\n14\n4\n5\n2\n1\n52\n29\n132\n66\n188\n\n\nreg_6368\n1\n1\n15\n17\n6\n10\n6\n1\n11\n5\n11\n3\n11\n23\n56\n\n\nreg_6369\n184\n304\n215\n661\n278\n519\n466\n435\n500\n267\n1045\n297\n1950\n1562\n3860\n\n\nreg_6370\n28\n1\n2\n92\n15\n21\n19\n50\n49\n1\n57\n9\n91\n44\n136\n\n\nreg_6371\n141\n290\n281\n673\n196\n398\n438\n303\n451\n188\n666\n222\n1793\n1409\n3416\n\n\nreg_6372\n1\n1\n3\n26\n3\n1\n1\n1\n1\n1\n39\n1\n39\n25\n222\n\n\nreg_6373\n63\n86\n112\n293\n70\n168\n55\n99\n268\n135\n187\n31\n580\n229\n894\n\n\n\n\n\n\n\n\nWe can then check the shape of our counts table (i.e. how many different peaks we are detecting and how many different samples?)\n\n# How many rows and columns does our count table have?\ndim(counts)\n\n[1] 1581   15\n\n\nWe can see that our table contains count information for 1581 peaks and 15 samples.\nWe can also inspect the metadata from the samples which is stored in the samples variable we created above.\n\n# What does the table look like?\nsamples\n\n\n\n\n\n\n\n\ngroups\nsizeFactor\nTissue\nSampleID\nTreatment\n\n\n\n\nSQ_2157\nK_org\n2.489936\n4KE\nSQ_2157\nK_org\n\n\nSQ_1990\nK_org\n2.758060\n8KE\nSQ_1990\nK_org\n\n\nSQ_2010\nK_org\n2.760352\n10KE\nSQ_2010\nK_org\n\n\nSQ_2163\nK_org\n2.425892\n13KE\nSQ_2163\nK_org\n\n\nSQ_2204\nK_org\n1.863534\n18KE\nSQ_2204\nK_org\n\n\nSQ_2212\nK_org\n2.580778\n11KW\nSQ_2212\nK_org\n\n\nSQ_2216\nK_org\n2.159358\n24KE\nSQ_2216\nK_org\n\n\nSQ_2222\nK_org\n2.203927\n22KE\nSQ_2222\nK_org\n\n\nSQ_2288\nK_org\n2.402964\n36KE\nSQ_2288\nK_org\n\n\nSQ_2303\nK_org\n2.430445\n41KE\nSQ_2303\nK_org\n\n\nSQ_2298\nN_crypts\n3.756529\nCR_41_mp\nSQ_2298\nN_crypts\n\n\nSQ_2145\nN_crypts\n2.804850\nCR_28_mp\nSQ_2145\nN_crypts\n\n\nGSM2058021\nN_crypts\n1.252530\nCR_28\nGSM2058021\nN_crypts\n\n\nGSM2058022\nN_crypts\n1.000000\nCR_29\nGSM2058022\nN_crypts\n\n\nGSM2058023\nN_crypts\n2.476903\nCR_37\nGSM2058023\nN_crypts\n\n\n\n\n\n\n\n\n\n# What is the shape of this samples table?\ndim(samples)\n\n[1] 15  5\n\n\nIn this case, this samples table has as many rows (15) as there are samples (which in turn is equal to the number of columns in the counts table), with columns containing different types of information related to each of the samples in the analysis."
  },
  {
    "objectID": "pages/Day1.html#savingloading-files",
    "href": "pages/Day1.html#savingloading-files",
    "title": "Day 1",
    "section": "",
    "text": "Let’s save this object with samples information in a file on this cloud session, this might be needed later if we end up in some trouble with the R session! This is a file format where columns are separated by commas. You might be familiar with this format if you have worked quite a bit in Excel. In R, we can save tabular data with the write.table() function specifying the location (the file name) we want. This is useful in the case our R session dies or we decide to interrupt it. In this case we will not have to run the whole analysis from the beginning and we can just source the file and load it!\n\nwrite.table(samples, \"samples_table.csv\", sep = \",\", quote = FALSE)\n\nWe can load the object back into the current session by using the following code line:\n\nsamples &lt;- read.table(\"samples_table.csv\", sep = \",\")\n\n\nWe will also repeat this procedure with the results of the differential expression analysis in order to avoid repeating work we have already done in case of any trouble!"
  },
  {
    "objectID": "pages/index.html",
    "href": "pages/index.html",
    "title": "Workshop Introduction",
    "section": "",
    "text": "Welcome! This workshop has been created as part of the Unimi course “Molecular Biology applied to Biotechnology”. The aim is to introduce you to the world of bioinformatics and to some of the most important concepts related to the analysis of Next Generation Sequencing (NGS) data.\nIn particular, you will be guided through the main steps of a ChIP-seq experiment, starting from the experimental design and its major challenges and then diving into a ChIP-seq analysis workflow! The hands-on part is based on R and for this reason an essential introduction to this programming language will be provided as well.\nThe dataset used in this workshop is taken from our study published on Nature Communications on 2021, “Epigenomic landscape of human colorectal cancer unveils an aberrant core of pan-cancer enhancers orchestrated by YAP/TAZ”. Part of the adventure will be dedicated to trying to reproduce some relevant analyses and plots published as results!\n\nLearning Objectives\n\nGet an overview of the ChIP-seq assay and its significance in genomic research\nUnderstand applications and key insights derived from ChIP-seq experiments\nFamiliarize with basic R functionalities\nDescribe the core steps of a ChIP-seq data analysis pipeline\nLearn how to navigate and utilize public resources for the analysis and exploration of genomic data\n\n\n\nWorkshop Schedule\nThis workshop is intended as a three-day tutorial. Each day will be dedicated to specific activities:\n\nDay 1\n\nIntroduction to ChIP-seq\nSetting up R environment\nUnderstanding ChIP-seq (and NGS) data core processing\nDataset introduction and exploration\n\n\n\nDay 2\n\nData normalization with edgeR\nDiagnostic and exploratory analysis\nDifferential analysis for ChiP-seq data\nVisualization of the results\n\n\n\nDay 3\n\nDownstream analyses on interesting subset of data, including:\n\nGene ontology with gProfiler\nMotif Analysis with MEME\n\nExploration of publicly available resources for accessing genomic data\n\n\n\n\nUs\n🧑🔬 Jacopo Arrigoni (jacopo.arrigoni@ifom.eu)\n👩💻 Carolina Dossena (carolina.dossena@ifom.eu)\n\n\nCredits\nThis workshop was inspired by other tutorials on ChIP-seq data analysis (the Bioconductor course, the teaching material from the HBC training and the tutorial from UCR. Mattia Toninelli (mattia.toninelli@ifom.eu) helped with the development of this site. The design of the analyses and the codes have been generated together with Michaela Fakiola (michaela.fakiola@ifom.eu).\n\n\nLicense\nAll of the material in this course is under a Creative Commons Attribution license (CC BY 4.0) which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  }
]